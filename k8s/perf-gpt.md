온프레미스 Kubernetes 클러스터 성능 테스트 및 카오스 엔지니어링 계획

1. 사전 준비 및 고려 사항
	•	클러스터 환경 확인: 주어진 온프레미스 Kubernetes 클러스터는 190개 노드(컨트롤 플레인 포함)로 구성되며, 각 노드에 매우 높은 사양의 하드웨어가 장착되어 있습니다 (Xeon Gold 6542Y 24코어 * 2, 메모리 768GB, SSD 스토리지 등). 운영체제는 RHEL 10이며 CNI로 Cilium을 사용하고 BGP를 통해 네트워크를 구성합니다. 이러한 환경 정보를 토대로 성능 테스트와 카오스 실험 시 노드 간 네트워크, 스토리지 I/O, 스케줄링 성능 등에도 영향이 있을 수 있으므로 미리 고려해야 합니다.
	•	네트워크 구성 점검: Cilium의 BGP 기능을 활성화하여 Pod/서비스 네트워크를 외부에 광고하고 있는 경우, 외부 라우터와의 BGP 피어링이 정상적인지 확인합니다 ￼. 이는 클러스터 외부에서 서비스에 접근할 수 있도록 해주는 중요한 설정이며, 성능 테스트 전에 BGP 경로 설정이 올바른지 (예: 서비스의 LoadBalancer IP가 BGP를 통해 광고되어 외부에서 접근 가능한지) 검증해야 합니다. 또한 Cilium (eBPF 기반)은 고성능을 제공하지만, 초기 설정 단계에서 네트워크 MTU, 모드(e.g., direct routing) 등이 적절히 구성되었는지 확인하여 최대 성능을 낼 수 있도록 합니다. 필요하다면 iperf3 등의 도구로 노드 간 네트워크 대역폭을 사전에 측정해 baseline으로 삼을 수 있습니다.
	•	모니터링 설치: 성능 테스트와 카오스 테스트 모두 모니터링 환경이 필수입니다. 클러스터에 Prometheus와 Grafana 등을 설치하여 CPU, 메모리, 네트워크, 디스크 I/O 등의 자원 사용량과 애플리케이션 메트릭(요청 처리량, 응답시간, 에러율)을 실시간으로 수집합니다. 특히 데이터 파이프라인 워크로드의 **정상 상태(steady state)**를 정의할 수 있는 지표들을 선정해야 합니다. 예를 들어 “초당 처리 레코드 수”, “평균 응답 시간”, “오류율” 등을 정상 상태의 기준으로 삼고, 이후 실험에서 이들 지표가 어떻게 변화하는지 모니터링할 것입니다.

2. 성능 테스트 계획

목표: 클러스터에 배포된 데이터 파이프라인 워크로드의 성능 한계를 파악하고, 부하 증가에 따른 응답 시간, 처리량(throughput), 자원 사용량 변화를 측정합니다. 여기에는 최대 처리 가능한 트래픽, 응답 시간 SLA 준수 여부, 시스템 병목 지점 파악 등이 포함됩니다. 또한 현재 네트워크 구조(Cilium+BGP)가 성능에 미치는 영향도 관찰합니다.

2.1 성능 테스트 대상 및 시나리오 설계
	1.	성능 지표 정의: 데이터 파이프라인의 특성에 맞는 핵심 지표(KPI)를 선정합니다. 예를 들어, 최대 처리율 (초당 처리 레코드 수 또는 요청 수), 평균/99퍼센타일 응답시간, 에러 발생률, 리소스 사용량(CPU, 메모리, 디스크 IOPS, 네트워크 대역폭) 등을 정의합니다. 또한 일정 시간 지속 부하 시 안정성(메모리 누수나 오류 발생 여부)도 확인합니다.
	2.	테스트 시나리오: 실제 운영을 고려하여 다양한 부하 패턴을 시나리오로 만듭니다. 예를 들어:
	•	증가 부하 테스트(Stress Test): 초기에는 낮은 부하로 시작해 점진적으로 요청/데이터 입력량을 늘리면서 시스템이 처리량을 얼마나 따라가는지 관찰합니다. 이때 임계점 이상에서는 응답시간이 급격히 증가하고 처리량 증가세가 둔화될 것으로 예상되는데, 이 지점을 찾아내는 것이 중요합니다 ￼.
	•	최대 부하 테스트(Spike/Burst Test): 짧은 시간 매우 높은 부하를 가해 피크 처리 능력을 측정합니다. 시스템 큐잉이나 배압(backpressure) 동작도 여기서 관찰합니다.
	•	지속 부하 테스트(Soak Test): 일정한 부하를 장시간(예: 수시간 이상) 유지하여 메모리 누수나 성능 저하가 발생하지 않는지 확인합니다.
	•	스케일링 테스트: 워크로드가 자동 확장(HPA 등)을 사용한다면 부하 변화에 따라 자동으로 파드/노드가 스케일 업/다운 되는지, 그리고 그 과정의 성능과 안정성을 측정합니다.
	3.	데이터 파이프라인 워크로드 준비: 성능 테스트는 실제 서비스와 유사한 환경에서 실행해야 의미 있는 결과를 얻을 수 있습니다. 실제 데이터 파이프라인 애플리케이션(예: 스트리밍 처리, ETL 배치작업 등)이 이미 있다면 해당 애플리케이션에 테스트용 입력 데이터를 주입하는 방식을 취합니다. 만약 표준적인 벤치마크를 원한다면, Apache Spark나 Flink 같은 엔진의 벤치마크, Kafka 스트리밍의 퍼포먼스 테스트 등 데이터 파이프라인 유형에 맞는 샘플 워크로드를 배포할 수 있습니다. 준비된 워크로드는 클러스터 내에 배포하고, 필요하다면 데이터 생성기/프로듀서도 함께 배포하여 파이프라인에 부하를 줄 수 있도록 합니다. (예: Kafka 기반 파이프라인이라면 Kafka Producer를 이용해 초당 N건의 메시지를 발행하고, Consumer 처리율을 측정하는 시나리오 등)

2.2 성능 테스트 도구 선정 (오픈소스)

온프레미스 환경에서 사용 가능하고 업계에서 많이 쓰이는 오픈소스 성능 테스트 도구를 선정합니다. 각 도구의 특징과 사용성, 확장성을 고려해 결정합니다:
	•	Apache JMeter: 가장 오래되고 인기 있는 오픈소스 부하 테스트 툴로, 다양한 프로토콜을 지원하고 성숙한 기능을 갖추고 있습니다 ￼. Java GUI로 시나리오를 작성하며 원래 LoadRunner 상용 툴의 대안으로 개발되었기에 전문적인 부하 테스트 기능들을 제공합니다 ￼. 여러 프로토콜(HTTP, gRPC, DB JDBC, FTP, SOAP/REST 웹서비스 등)과 레코딩 기능까지 지원하여 복잡한 시나리오 구성도 가능합니다 ￼. 다만 JMeter는 스레드 기반 아키텍처로 동작하여 부하량이 커지면 부하 발생기 자체에 많은 리소스를 소비할 수 있고, 대규모 테스트 시 여러 대의 부하 발생기를 클러스터로 구성하는 설정이 필요 등 **확장(스케일)**에 다소 어려움이 있습니다 ￼. 190노드 규모의 큰 테스트 환경에서는 JMeter로 수십만 RPS 부하를 내려면 마스터/슬레이브 노드 구성과 분산 설정에 상당한 노력이 필요할 수 있습니다.
	•	Locust: Python으로 구현된 가볍고 사용하기 쉬운 분산 부하 테스트 도구입니다 ￼. Locust에서는 부하 시나리오를 일반 파이썬 코드로 작성하므로 개발자 친화적이며, 이벤트 기반(asynchronous)으로 가상 유저를 처리하므로 동일 부하에서 JMeter 대비 약 70% 적은 리소스만 사용할 정도로 효율적입니다 ￼. 이러한 이벤트 기반 접근 덕분에 한 개의 Locust 프로세스도 수천~수만의 동시 유저를 시뮬레이션할 수 있어 고도로 스케일 아웃하기에 적합합니다. 또한 웹 기반 UI 대시보드를 제공하여 테스트 진행 중 실시간으로 현재 RPS, 응답시간 분포, 활성 유저 수 등을 모니터링할 수 있고, 테스트 중 동적으로 부하 수준(유저 수 증가/감소)을 조절할 수도 있습니다 ￼. 웹 UI를 통해 각 API 요청별 평균/백분위 응답시간, 초당 요청수, 에러율 등을 한눈에 볼 수 있어 결과 분석에 용이합니다. Locust는 여러 노드에 마스터-워커 분산 실행을 지원하므로, 부하 발생 전용으로 몇몇 노드를 활용해 클러스터 내부 또는 외부에서 대규모 부하를 발생시키는 구조로 사용합니다.
	•	Grafana k6: k6는 Go로 작성된 개발자 중심의 모던 성능 테스트 도구로, JavaScript 스크립팅을 통해 부하 시나리오를 코드로 작성하고 CLI로 실행하는 방식입니다 ￼. k6 자체는 가벼우면서도 높은 부하 생성이 가능하고, 내장된 기능으로 성능 테스트를 CI 파이프라인에 통합할 수도 있습니다 ￼. (예: 코드 리포지토리에 성능 테스트를 포함하고, 빌드 시 자동 실행하여 성능 회귀를 검출) 2023년부터는 xk6-browser 같은 확장을 통해 브라우저 레벨 성능 측정까지도 가능해졌을 정도로 발전했습니다 ￼. k6는 CLI 모드로 동작하며 결과를 터미널 및 Grafana와 연동된 시간별 메트릭으로 제공하므로, Grafana 대시보드로 테스트 결과를 시각화하며 분석할 수 있습니다. (Grafana에서 k6용 대시보드 및 Prometheus 수집도 지원) k6 역시 분산 실행이나 Kubernetes 연동(컨테이너로 k6 Pod를 여러 개 돌려 부하 생성) 등이 가능하므로, 대규모 부하에도 적합합니다 ￼.
	•	기타 도구: 그 외에 고려할 수 있는 오픈소스 도구로 nGrinder(네이버에서 개발한 엔터프라이즈급 분산 부하테스트 플랫폼)가 있습니다. nGrinder는 JMeter나 Grinder 엔진을 기반으로 하며, 웹 인터페이스에서 시나리오 작성(Jython/그루비 사용) 및 대시보드 모니터링을 제공하여 테스트 생성실행모니터링~결과수집을 일원화한 솔루션입니다 ￼ ￼. 조직 내 다수 사용자와 공유하며 쓰기에 좋지만, 러닝커브가 있을 수 있습니다. 또한 Gatling (Scala 기반의 고성능 부하툴 ￼), The Grinder (Jython 지원), Tsung (Erlang 기반 다프로토콜 부하툴) 등도 있지만 상대적으로 국내 활용도가 낮고 학습이 필요하므로 주요 옵션으로는 JMeter, Locust, k6를 권장드립니다. 이 셋은 모두 오픈소스이고 전 세계적으로 널리 쓰이며, 온프레미스에서도 제약 없이 사용 가능합니다.

도구 선정 요약: 현업에서 가장 많이 쓰이는 부하테스트 도구로는 JMeter, Locust, k6 등이 있으며 ￼ ￼, 이 중 사용성과 확장성 측면에서 Locust나 k6가 특히 추천됩니다. JMeter는 기능이 풍부하지만 대규모 테스트에서 부하발생기 자체가 병목이 될 수 있으므로, 효율적 부하 생성을 위해 **Locust (경량, 이벤트기반)**를 주력으로 사용하고 결과를 Locust Web UI와 Grafana로 모니터링하는 방안을 고려합니다. 동시에 k6를 이용하여 CI 파이프라인에 성능테스트를 통합하거나 특정 API의 세밀한 부하 테스트를 스크립팅해볼 수 있습니다. 선택한 도구에 익숙한 팀원이나 기존에 사용 경험이 있는지를 고려하여 최종 결정하면 됩니다.

2.3 성능 테스트 수행 단계
	1.	베이스라인 수집: 테스트에 앞서 현재 시스템의 베이스라인 성능을 측정합니다. 매우 낮은 부하(예: 1분당 몇 건 수준)에서 응답시간, 자원 사용량 등을 측정해 둡니다. 또한 데이터 파이프라인의 정상 처리 성능(예: 평상시 평균 처리율)을 기록해두고, 이때의 시스템 상태(리소스 여유도 등)를 파악합니다.
	2.	부하 발생 및 모니터링: 앞서 정의한 시나리오에 따라 부하를 발생시킵니다. Locust를 사용하는 경우, 마스터/워커로 구성하여 워커 수를 점진적으로 늘리거나 Locust의 UI/CLI 옵션으로 **동시 유저 수(users)와 상승 속도(spawn rate)**를 조절하면서 부하를 증대시킵니다. 예컨대 매 30초마다 50명씩 가상 유저를 추가하여 부하를 올리는 식으로 진행합니다. 테스트 동안 Grafana 대시보드나 Locust Web UI를 통해 실시간 지표를 모니터링합니다 ￼. 특히 응답시간 분포가 급격히 변하는 지점, 에러가 발생하기 시작하는지, 처리량이 더 이상 늘지 않고 plateau에 이르는 시점 등을 주시합니다. 아래 그림은 Locust를 통해 실시간으로 **RPS(초당 요청 수)**와 응답시간 변화, 활성 사용자 수를 관찰한 예시입니다. 파란 선은 활성 유저 수 증가를, 초록 선은 처리 중인 RPS를, 주황/보라 선은 응답시간의 변화를 보여줍니다 ￼. 일정 유저 수를 넘어서면서부터 요청 처리율(RPS)은 더 이상 증가하지 않고 응답시간은 급격히 악화되는 모습을 볼 수 있는데, 이런 그래프가 나타나는 지점이 해당 시스템의 용량 한계(bottleneck)임을 알 수 있습니다 ￼. 이러한 한계점을 찾아내어 최대 처리량과 그때의 응답시간을 기록합니다.

예시: Locust 웹 UI를 통한 부하 테스트 모니터링. 활성 사용자 증가에 따라 초당 처리량(RPS)과 응답시간 지표가 실시간으로 표시된다. 그래프에서 일정 시점 이후 RPS가 증가 정체되고 응답 시간이 악화되면 시스템이 과부하 상태에 진입했음을 의미한다 ￼.
	3.	임계점 분석: 부하 수준을 조금씩 높여가며 임계 부하점을 찾습니다. 예를 들어 동시 처리 N건까지는 평균 응답시간이 200ms 이내로 유지되다가, N+∆ 시점에서 응답시간이 SLA(예: 1초)을 넘기기 시작하고 에러율이 증가한다면, N 건이 이 시스템의 현재 처리 한계로 간주할 수 있습니다. 임계점을 넘은 부하에서는 큐잉이나 시간초과가 발생하는지 관찰하고 관련 로그를 수집합니다.
	4.	병목 원인 파악: 최대 부하 구간에서 시스템 병목이 어디에서 발생하는지 조사합니다. 예를 들어 CPU가 100% 치솟는지, GC가 빈번히 도는지, 디스크 I/O 대기가 많은지, 네트워크 대역폭이 포화되는지 등을 모니터링 데이터로 분석합니다. 필요 시 각 마이크로서비스별 APM 로그나 메트릭(New Relic, Jaeger 등)으로 상세 분석을 수행합니다. 데이터 파이프라인이라면 특정 단계(예: 데이터 변환 로직, 또는 외부 DB 쓰기 등)가 느려지는지 추적합니다.
	5.	시나리오별 반복 테스트: 정의한 여러 시나리오(스파이크, 장기부하 등)를 순차적으로 실행하여 데이터를 축적합니다. 장기 안정성 테스트의 경우, 몇 시간 동안 부하를 유지한 후 메모리 누수나 성능 저하 징후가 없는지 확인합니다. 또한 스파이크 테스트로 순간 트래픽 급증 시 시스템이 자동 확장으로 대응했는지, 아니면 요청 일부를 놓쳤는지 등을 평가합니다.
	6.	결과 분석 및 튜닝: 테스트 종료 후 수집된 모든 결과를 종합해 성능 프로파일을 작성합니다. 여기에는 “현재 시스템은 초당 XX건 처리에서 응답시간 YYms를 유지하지만, 그 이상에서는 큐 대기 발생”, “네트워크 대역폭은 노드당 최대 ZZ Gbps까지 사용됨”, “CPU 사용률 80%를 넘기면서부터 처리율 증가세가 둔화됨” 등의 관찰을 포함합니다. 또한 발견된 병목에 대한 튜닝 방안을 제안합니다. (예: 애플리케이션 파라미터 조정, 리소스 할당 증량, 스케일아웃 등) 성능 튜닝 후에는 동일 테스트를 다시 실행해 개선 여부를 확인하는 피드백 사이클을 거칩니다.

3. 카오스 엔지니어링 계획

목표: 프로덕션 환경에서 발생할 수 있는 다양한 장애 상황을 의도적으로 주입하여 시스템의 복원력(resilience)을 검증하고, 서비스별 **단일 장애점(SPOF)**이 존재하는지 식별합니다. 쿠버네티스 기반 환경에서는 일반적으로 Pod 장애, 노드 장애, 네트워크 장애 등이 주요 가용성 위협인데, 이러한 상황에서 데이터 파이프라인 서비스가 얼마나 견고하게 유지되는지 테스트합니다. 카오스 엔지니어링을 통해 미처 몰랐던 취약점을 발견하고 사전에 보완하는 것이 목적입니다.

3.1 카오스 실험 준비 및 시나리오
	1.	SPOF 식별 및 가설 수립: 먼저 현재 시스템 아키텍처에서 잠재적 단일 장애점들을 찾아냅니다. 예를 들어:
	•	특정 핵심 마이크로서비스가 단일 Pod 인스턴스로만 구성되어 있다면 그것이 SPOF입니다. 또한 StatefulSet으로 동작하는 DB나 메시지 큐 등이 하나의 인스턴스만 존재하거나 Active-Standby 구성이라면 장애 시 서비스 중단 가능성이 있습니다.
	•	데이터 파이프라인의 경우, 데이터 소스나 싱크가 되는 외부 시스템(예: DB, 파일시스템)이 단일 지점인지, 또는 Kafka 같은 메시지 브로커의 파티션 리더가 한 노드에 몰려 있지는 않은지 등을 점검합니다.
이 단계에서 “어떤 장애를 주었을 때 시스템이 정상 상태(steady state)에서 벗어나 어떤 증상이 날까”에 대한 가설을 세웁니다. 예를 들어 “만약 워크플로우 중간 처리 역할의 서비스 Pod 하나를 강제로 죽여도 다른 복제본이 있어서 서비스 중단이 없을 것이다”라는 가설을 세울 수 있습니다.
	2.	카오스 실험 시나리오 정의: 다양한 장애 상황을 재현하기 위한 시나리오를 작성합니다. 각 시나리오는 하나의 가설 검증에 대응하도록 설계합니다. 주요 시나리오 예시는 다음과 같습니다:
	•	Pod 장애 실험: 임의의 애플리케이션 Pod를 강제 종료하거나 삭제합니다. 일반 Deployment로 복제본이 2개 이상이라면 남은 Pod로 트래픽이 정상 처리되어야 하며, Kubernetes가 자동으로 새 Pod를 생성해 원래 복제 수를 복구하는지 확인합니다. 이 실험을 통해 서비스 메쉬 또는 로드밸런싱이 제대로 동작하여 트래픽이 살아남은 인스턴스로 바로 우회되는지, 클라이언트에는 장애 영향이 없었는지 관찰합니다. 만약 복제본이 1개 뿐인 서비스라면 해당 서비스는 잠시 중단되어 SPOF임이 드러날 것이므로, 이 경우 레플리카 수 증가 등의 조치가 필요합니다.
	•	노드 장애 실험: Kubernetes 노드 하나를 종료 또는 격리시켜 봅니다. 방법은 실제 물리 노드 전원을 내리는 대신, 카오스 도구를 통해 그 노드의 kubelet이나 네트워크를 차단하는 방법을 사용합니다. 노드가 정상 동작하지 않으면, 그 노드에 있던 모든 Pod는 일정 시간 후 다른 노드로 스케줄되어 재시작되어야 합니다. 이때 데이터 파이프라인의 상태도 관찰합니다. (예: 특정 노드에 있던 Kafka 브로커나 Spark Executor가 죽었다면 리밸런싱이나 작업 재시도가 이루어지는지) 또한 노드 장애 시 ETCD 및 컨트롤 플레인에는 영향이 없었는지도 확인합니다. (컨트롤 플레인 노드들도 여러 대冗余구성일 것이므로 하나 정도 장애는 허용되어야 함)
	•	네트워크 지연/단절 실험: 특정 서비스 간 통신에 **인위적인 네트워크 지연(latency)**을 주거나 패킷 손실을 발생시킵니다. 예를 들어, 데이터 파이프라인의 Producer -> Consumer 경로에 100ms 지연을 삽입해보거나, 혹은 특정 Pod의 네트워크를 완전히 drop시켜 네트워크 파티션 상황을 만듭니다. 이를 통해 애플리케이션 레벨에서 타임아웃 처리, 재시도 로직 등이 정상 작동하여 시스템이 부분 장애를 견뎌내는지 확인할 수 있습니다. (예: Consumer가 일시적으로 Kafka에 연결 안될 때 재시도하는지, 또는 API 통신 시 timeout 설정이 적절한지)
	•	리소스 고갈 실험: 대상 노드나 Pod에 CPU or 메모리 부하를 인위적으로 발생시켜 봅니다. 예를 들어 카오스 실험으로 한 노드의 CPU를 100% 점유하게 만들면, 그 노드에 있는 모든 Pod들의 응답이 느려질 수 있습니다. 이때 HPA(Horizontal Pod Autoscaler)가 응답시간 증가를 감지해 자동으로 인스턴스를 늘리는지, 혹은 노드 자체에 대한 Cluster Autoscaler가 작동해 신규 노드를 추가하려 하는지 등을 볼 수 있습니다. 메모리 부족 상황을 만들어 OOM이 발생하면 Pod이 재시작되는데, 이러한 상황에서 데이터 손실이나 상태 꼬임이 없는지도 검사합니다.
	•	특화된 장애: 데이터 파이프라인 구성 요소별로 특화된 장애도 고려합니다. 예를 들어 Kafka를 쓴다면 Kafka 브로커 하나를 kill해서 남은 브로커로 클라이언트가 자동 재시도하는지, Spark라면 Executor 프로세스 kill 등을 시도해 잡(job)이 재시도되어 성공하는지 등을 테스트합니다. 또한 Cilium+BGP 환경을 사용하므로, 만약 서비스 LB IP 광고에 BGP를 쓰고 있다면 외부 라우터와의 BGP 세션 down 상황도 실험해볼 수 있습니다. (한 라우터와의 연결이 끊겨도 다른 경로로 트래픽이 흐르는지 등) 다만 네트워크 인프라 레벨의 실험은 실제 환경 영향이 크므로 필요한 경우에 한정합니다.
	3.	안전장치 및 범위 설정: 카오스 실험은 실제 장애를 내는 것이므로 주의 깊은 진행이 필요합니다. 각 실험 시나리오에는 Blast Radius(폭파 범위)를 정의하여, 영향이 국한되도록 합니다. 예컨대 한번에 하나의 노드, 하나의 서비스만 실험하도록 하고, 프로덕션 전체를 대상으로 하지 않도록 설정합니다. 또한 실험 실행 전 현재 시스템이 **정상 상태(steady state)**인지 확인해야 합니다. 모니터링 지표상 오류가 있거나 트래픽이 비정상적이면 실험을 미룹니다. 실험 중에도 실시간으로 서비스 상태를 관찰하며, 예상치 못한 심각한 문제가 발생하면 즉시 중단할 수 있는 준비(예: kubectl undo 나 자동 롤백 스크립트)를 해둡니다. 카오스 엔지니어링 원칙 상, “실험은 통제 가능한 범위 내에서 점진적으로” 이루어져야 합니다.

3.2 카오스 테스트 도구 선정 (오픈소스)

쿠버네티스 환경에서 널리 쓰이는 카오스 엔지니어링 오픈소스 플랫폼을 활용하여 위 시나리오를 구현합니다. 주요 후보로 LitmusChaos와 Chaos Mesh가 있으며, 둘 다 CNCF에서 주도하고 있는 프로젝트로 시장 활용도와 완성도가 높습니다 ￼. 이들 도구는 쿠버네스에 CRD(Custom Resource) 및 컨트롤러를 설치하여, 사용자가 정의한 Chaos 실험을 자동으로 실행하고 결과를 수집해 줍니다.
	•	LitmusChaos: MayaData(현재 ChaosNative)에서 시작한 쿠버네티스 네이티브 카오스 테스팅 프레임워크로, CNCF Sandbox에 채택될 정도로 많은 기업에서 사용 중인 성숙한 도구입니다 ￼. Litmus는 쿠버네티스에서 동작하며, ChaosExperiment, ChaosEngine 등의 CRD를 통해 어떤 실험을 어느 대상에 할지 선언적으로 정의합니다. Litmus의 강점은 **광범위한 실패 시나리오 라이브러리(ChaosHub)**를 제공한다는 점입니다. 공개 ChaosHub 레지스트리에는 네트워크 지연, Pod/노드 강제종료, 컨테이너 CPU 소진, 디스크 오류, 쿠버네티스 자원(quota) 고갈 등 다양한 실험 템플릿이 준비되어 있어 필요 시 가져다 사용할 수 있습니다 ￼. 예를 들어 “Pod Delete” 실험이나 “Node CPU Hog” 실험 등을 바로 적용 가능하며, 사용자는 해당 실험의 파라미터(대상 앱 라벨, 지속 시간 등)만 지정하면 됩니다. Litmus의 또 다른 장점은 **웹 UI(Litmus Portal, ChaosCenter)**와 API를 제공하여 사용성이 높다는 것입니다 ￼. ChaosCenter라는 중앙 대시보드를 통해 실험을 스케줄링하고, 진행 상황을 모니터링하며, **실험 결과(resilience score 산출 등)**를 한 눈에 볼 수 있습니다. 또한 CI/CD 파이프라인에 Chaos 테스트를 통합할 수 있는 API 및 리포팅 기능이 잘 갖춰져 있어, 지속적 혼돈 테스트를 구현하기에도 용이합니다 ￼.
	•	Chaos Mesh: PingCAP에서 시작하여 CNCF Incubating 프로젝트로 발전한 쿠버네티스 전용 카오스 엔지니어링 플랫폼입니다 ￼. Litmus와 유사하게 쿠버네티스 CRD(ChaosExperiment 개념)에 기반하여 동작하지만, Chaos Mesh는 특히 경량성과 개발자 친화적인 UI로 인기가 높습니다. Chaos Mesh 역시 여러 가지 실패 유형을 지원하는데, **기본 자원 레벨의 장애(DNS 혼란, 파일시스템 오류)**부터 플랫폼 레벨(AWS/GCP VM 장애 등), 애플리케이션 레벨(JVM 에러 주입) 등 세 가지 범주로 실험을 분류합니다 ￼. 예를 들어 NetworkChaos(지연, 패킷손실), PodChaos(Pod kill, Pod 강제중지), StressChaos(CPU/Memory 사용률 상승) 등의 리소스별 실험 CRD를 제공하며, YAML로 간단히 어떤 Pod에 얼마나 지연을 줄지 등을 정의할 수 있습니다. Chaos Mesh는 Chaos Dashboard라는 웹 UI도 제공하여, 사용자가 드래그앤드롭으로 워크플로우 형태로 여러 실험을 연결해 시나리오를 구성할 수도 있습니다 ￼. (예: 먼저 Pod 하나 종료 -> 그 후 네트워크 지연 주는 식으로 연쇄 실험) 또한 GitHub Actions와 연동하는 등 CI 파이프라인 통합도 지원합니다 ￼.
	•	기타 도구:
	•	ChaosBlade: 알리바바에서 만든 오픈소스 카오스 도구로, 물리 호스트, 쿠버네티스, Java 애플리케이션 세 레벨에 대해 혼돈 주입을 지원합니다 ￼. CLI 중심 도구이지만 ChaosBlade Box라는 웹 UI도 제공되며, Litmus나 Chaos Mesh에 비해 저수준(host, JVM) 공격에 강점이 있습니다. 예를 들어 리눅스 커널 레벨에서 패킷 드롭, JVM 바이트코드 조작으로 Exception 발생 등이 가능합니다 ￼.
	•	Kube-monkey: 넷플릭스 Chaos Monkey 아이디어를 Kubernetes에 적용한 경량 도구로, 주기적으로 무작위 Pod를 종료하는 방식으로 동작합니다 ￼. 매일 일정 시간대에 활성화되어 임의의 대상(특정 라벨이 붙은 디플로이먼트 중) Pod를 kill하고 종료 로그를 남기는 식이라 지속적인 불규칙 장애를 주입하여 시스템이 항상 장애 내성을 갖추도록 훈련시킵니다. ChaosKube, Chaos Monkey for K8s 등도 유사하게 랜덤 Pod 삭제에 특화된 도구입니다. 이러한 도구들은 특정 시나리오보다는 지속적인 무작위 장애 테스트에 유용합니다.
	•	Chaos Toolkit: 오픈소스 Python 기반 프레임워크로, JSON/YAML로 실험 시나리오를 스크립팅하여 쿠버네티스와 클라우드 자원에 혼돈을 줄 수 있습니다. Driver를 통해 각종 플랫폼에 연결하고, 실험 단계를 코드로 세밀하게 짤 수 있는 장점이 있지만, 위의 Litmus/ChaosMesh 만큼 사용자 친화적 UI는 없고 스크립트 작성이 필요합니다.

도구 선정 요약: LitmusChaos와 Chaos Mesh 두 가지가 커뮤니티에서 많이 사용되고 지원도 활발하므로 권장됩니다 ￼. Litmus는 기업에서도 많이 채택하여 활용 중이고, 실험 시나리오 템플릿이 풍부하며 웹 UI로 팀원들이 손쉽게 활용하기 좋습니다 ￼ ￼. Chaos Mesh는 Kubernetes에 특화되어 가볍게 쓰기에 좋고, 주로 쿠버네티스 리소스 레벨의 장애 시나리오에 강점이 있습니다 ￼. 두 도구 모두 오픈소스로 온프레미스 클러스터에 설치하여 사용 가능합니다. 클러스터 규모가 크므로, Litmus/Chaos Mesh의 카오스 연산자(Operator) 및 관련 CRD를 설치할 때 리소스 제한이나 네임스페이스 범위를 고려해야 합니다. (예: 카오스 테스트 전용 네임스페이스에서 운영)  또한 대규모 환경에서는 한 번에 많은 Pod를 대상으로 실험하면 예기치 못한 파급효과가 있을 수 있으므로, 실험 대상 범위를 점진적으로 늘리는 전략이 필요합니다.

LitmusChaos 카오스 엔지니어링 플랫폼의 아키텍처 개요 ￼ ￼. 사용자는 ChaosCenter(UI 또는 API)를 통해 카오스 실험(ChaosFaults 시나리오)을 생성하며, Litmus 컨트롤 플레인에서 이를 접수하여 실행 플레인(각 대상 클러스터의 Chaos Operator 등)을 통해 실제 장애를 주입한다. 실험 결과는 다시 컨트롤 플레인으로 수집되어 대시보드나 외부 모니터링 도구(Prometheus/Grafana)로 분석 가능하다.

3.3 카오스 실험 진행 단계
	1.	실험 환경 설정: 선정된 카오스 도구(Litmus 또는 Chaos Mesh 등)를 클러스터에 설치합니다. 운영 중인 클러스터에 바로 설치하기 어려우면 동일한 설정의 스테이징/테스트 클러스터에 먼저 설치해보는 것도 좋습니다. 각 도구의 연산자(Operator)와 CRD가 잘 등록되었는지 확인하고, 웹 UI를 사용할 경우 포털 접속도 확인합니다. LitmusChaos의 경우 ChaosCenter 계정을 만들고 프로젝트를 설정하며, Chaos Mesh는 대시보드 GUI에 접속합니다.
	2.	실험 시나리오 구현: 앞서 정의한 카오스 시나리오들을 실제 실험 CRD(manifest) 또는 UI 워크플로우로 작성합니다. 예를 들어 LitmusChaos를 사용한다면 Pod Delete 실험 CRD를 생성하되, .spec.appinfo에 대상 앱 레이블과 Ns를 지정하고 .spec.experiments.spec.components.env에 강제종료할 시간 등 파라미터를 설정합니다. Chaos Mesh라면 NetworkChaos CRD를 YAML로 작성하거나 대시보드에서 필드 입력으로 구성할 수 있습니다. 각 실험은 개별적으로 실행 가능하도록 설계하고, 나중에 연계 실행(예: 연속된 장애 주기)도 고려한다면 워크플로우(ChaosScenario)를 정의합니다. 이때 각 실험의 중단 조건(Steady state 확인용 프로브)도 함께 설정합니다. 예컨대 Litmus에서는 Probes 기능으로, 실험 중/후에 특정 메트릭 쿼리(PromQL)나 HTTP 응답 체크 등을 수행해 서비스 정상 여부를 판단할 수 있습니다. 이러한 프로브를 통해, “장애 주입 후도 특정 API 헬스체크 응답이 200을 유지한다” 등을 자동 검증하도록 합니다.
	3.	사전 리허설: 중요한 프로덕션에 적용하기 전에, 가볍게 리허설 실험을 해봅니다. 예를 들어 테스트 환경에서 동일 실험을 돌려보거나, 프로덕션의 비업무 시간대에 트래픽이 거의 없는 서비스에 제한적으로 실행해봅니다. 이를 통해 실험이 의도한 대로 동작하는지, 과도한 영향은 없는지 확인합니다.
	4.	실험 실행: 각 카오스 실험을 순차적으로 실행합니다. 모니터링 시스템을 주시하여, 장애 주입 시각 전후로 시스템 메트릭과 서비스 지표가 어떻게 변하는지 면밀히 관찰합니다. 필요하면 Application 로그도 모아 봅니다. 예를 들어 Pod 강제 종료 실험의 경우, 해당 Pod를 경유하던 요청이 다른 Pod로 잘 우회되었는지, 사용자는 장애를 느끼지 못했는지 (에러율 증가 없음)을 확인합니다. 네트워크 지연 실험의 경우, 실제 서비스 응답시간이 인위적 지연만큼 증가했는지 (지연이 제대로 적용되었는지) 그리고 애플리케이션이 timeout을 견디는지 등을 봅니다. LitmusChaos의 결과 요약이나 Chaos Mesh 결과를 통해 실험 성공/실패 여부도 확인합니다. (예: Litmus에서는 ChaosResult 리소스에 실험이 성공(즉 시스템이 견뎠음)인지 실패(기대와 달리 문제 발생)인지 기록됨)
	5.	결과 및 대응 조치 분석: 각 실험이 끝난 후, 발견된 문제점을 분석합니다. 예를 들어 Pod 하나 종료 실험에서 짧은 순간이지만 사용자 요청에 오류가 발생했다면, 해당 서비스가 일시적인 failover 동안 견디지 못한 것으로 볼 수 있습니다. 그 원인이 무엇인지 (예: LB re-route 지연, 또는 상태 세션 문제 등) 찾아서 조치 방안을 논의합니다. 노드 장애 실험에서 특정 컴포넌트가 복구되지 않았다면 (예: StatefulSet이 자동 복구 안 됨 등) 해당 부분의 설정을 수정해야 합니다. 네트워크 장애 실험으로 어떤 마이크로서비스가 timeout 발생 후 재시도하지 않아 오류를 낸다면, 코드 수정이나 구성 조정을 검토합니다. 이러한 개선 조치들을 정리하고 우선순위를 매겨 수행합니다.
	6.	재실험 및 지속적 테스트: 수정/개선된 사항이 있다면 동일 카오스 실험을 반복 수행하여 문제가 해결됐는지 검증합니다. 예를 들어 이전에 실패했던 실험(Litmus 기준 ChaosResult에 Fail로 떴던)이 이제 Pass가 나오는지 확인합니다. 카오스 엔지니어링은 지속적인 과정이므로, 정기적으로 (예: 분기별 혹은 주요 배포 전) 중요 시나리오에 대한 카오스 테스트를 실행하는 계획을 수립합니다. 나아가 CI 파이프라인에 카오스 테스트를 포함시켜, 새로운 배포나 업데이트 시 기본적인 혼돈 테스트가 자동으로 실행되어 회귀를 막을 수도 있습니다.

4. 추가 고려사항 (모니터링 및 문화)
	•	모니터링 통합: 성능 테스트 및 카오스 테스트의 결과는 일원화된 모니터링 대시보드로 볼 수 있도록 구성하는 것이 이해와 공유에 용이합니다. 예를 들어 Grafana에서 성능 테스트 시나리오별 대시보드를 만들고, 부하 테스트 동안의 시스템 지표와 애플리케이션 지표를 타임라인에 따라 표시합니다. 카오스 실험의 경우에도 실험 시작/종료 시간을 표시하고 그 구간의 지표 변화, 에러 발생 유무 등을 시각화합니다. LitmusChaos를 사용하면 Grafana에 Resilience Score나 실험 성공율 등을 보드로 나타낼 수도 있습니다.
	•	로그 및 추적: 부하 테스트 시 애플리케이션 로그 레벨을 일시적으로 상향하여 (debug 모드 등) 성능 병목 구간의 내부 동작을 분석하거나, 분산 트레이싱(Jaeger 등)을 통해 요청별 지연 구간을 파악하면 병목 원인 분석에 도움이 됩니다. 카오스 테스트 시에도 장애 발생 직전/직후 로그를 수집해두면 정확한 장애 영향 범위를 파악할 수 있습니다.
	•	조직 문화: 카오스 엔지니어링 실시는 팀의 공감대와 승인이 필수입니다. 실험 전에 관련 개발자, 운영팀과 상의하여 안전장치와 목적을 공유하고, 작은 성공 사례부터 시작해 신뢰를 쌓는 것이 좋습니다. 처음부터 프로덕션에 강도 높은 혼돈을 주입하기보다, 스테이징 환경에서 시작해 프로덕션에 적용 범위를 넓혀가는 단계적 접근을 권장합니다.
	•	종합 보고 및 피드백: 마지막으로, 성능 시험과 카오스 실험 결과를 종합한 보고서를 작성합니다. 여기에는 테스트 방법, 관찰된 수치와 현상, 개선이 필요한 부분, 조치 계획 등을 상세히 담아 향후 레퍼런스로 삼습니다. 이 보고서를 기반으로 시스템 튜닝과 아키텍처 개선 작업을 수행하고, 완료 후 다시 테스트하여 선순환을 구축합니다.

以上의 계획을 통해 온프레미스 Kubernetes 클러스터의 성능 한계를 파악하고, 잠재 장애 상황에 대한 대비책을 검증함으로써 신뢰성 높고 고성능인 데이터 파이프라인 플랫폼을 구현할 수 있을 것입니다. 필요한 추가 정보가 있으면 언제든지 문의해주시기 바랍니다. 🚀