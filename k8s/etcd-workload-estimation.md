# Airflow + Spark Operator 환경 etcd 부하 분석 보고서
## 워크로드 3배 증가 시나리오

---

## 📋 분석 환경

### Control Plane 노드 스펙

| 항목 | 스펙 |
|------|------|
| CPU | 48코어 (24C × 2 소켓) |
| RAM | 768GB |
| 스토리지 (SAS) | SAS SSD 800GB × 2 |
| 스토리지 (NVMe) | NVMe 3.84TB × 2 |
| 네트워크 | 10/25GbE 2포트 × 3 |

### 클러스터 구성

| 항목 | 값 |
|------|-----|
| 팀 수 | 4개 |
| 팀별 워크로드 | **60개** (기존 20개 × 3배) |
| 총 워크로드 | **240개** |
| 워커 노드 (가정) | 50개 |

---

## 📊 팀별 워크로드 구성

### 1개 팀 기준 (60개 워크로드)

| 유형 | 주기 | 개수 | Executor | 실행시간 | 시간당 실행 |
|------|------|------|----------|----------|------------|
| 🔥 **Heavy (Iceberg)** | 5분 | **3** | 10 | 4분 | 36.0 |
| 일반 배치 | 5분 | 9 | 3 | 2분 | 108.0 |
| 일반 배치 | 15분 | 9 | 4 | 3분 | 36.0 |
| 일반 배치 | 30분 | 9 | 5 | 5분 | 18.0 |
| 일반 배치 | 1시간 | 12 | 6 | 10분 | 12.0 |
| 일반 배치 | 6시간 | 9 | 8 | 20분 | 1.5 |
| 일반 배치 | 1일 | 9 | 15 | 60분 | 0.4 |
| **합계** | - | **60** | - | - | **211.9** |

### 4개 팀 합계

| 항목 | 값 | 기존 대비 |
|------|-----|----------|
| 총 워크로드 수 | 240개 | 3배 ↑ |
| 시간당 총 Job 실행 | **847.5 Jobs/hour** | 3배 ↑ |
| 시간당 Pod 생성/삭제 | **4,878 pods** | 3배 ↑ |

---

## 🔥 상위 부하 워크로드 (팀당)

| 순위 | 워크로드 | 개수 | ops/hour | 비중 |
|------|----------|------|----------|------|
| 1 | batch_job_group_1 (5분 주기) | 9개 | 71,280 | 37% |
| 2 | ⭐ heavy_iceberg_ingest | 3개 | 57,780 | 30% |
| 3 | batch_job_group_2 (15분 주기) | 9개 | 28,620 | 15% |
| 4 | batch_job_group_3 (30분 주기) | 9개 | 16,740 | 9% |
| 5 | batch_job_group_4 (1시간 주기) | 12개 | 12,780 | 7% |

> ⚠️ **5분 주기 워크로드 12개(Heavy 3 + 일반 9)가 전체 부하의 67%를 차지**

---

## 🔷 Control Plane 3노드 구성 분석

### etcd 부하 상세

| 부하 항목 | ops/hour |
|-----------|----------|
| Spark 워크로드 (쓰기) | 152,046 |
| Spark 워크로드 (읽기) | 608,184 |
| Node Heartbeat (53노드) | 19,080 |
| Spark Operator | 360 |
| Airflow Scheduler | 2,160 |
| Controller Manager | 1,500 |
| Endpoints | 3,000 |
| Watch Events | 6,000 |
| etcd Internal | 360 |
| **총 쓰기** | **161,784** |
| **총 읽기** | **630,906** |
| **총계** | **792,690** |

### 초당 환산

| 메트릭 | ops/sec |
|--------|---------|
| 쓰기 | 44.9 |
| 읽기 | 175.3 |
| **총계** | **220.2** |

### 성능 분석

| 항목 | 값 |
|------|-----|
| 추정 쓰기 처리량 (NVMe) | 30,000 ops/sec |
| 추정 읽기 처리량 | 137,500 ops/sec |
| 추정 총 처리량 | 167,500 ops/sec |
| 추정 디스크 지연 | 0.5 ms |
| **쓰기 부하 비율** | **0.15%** |
| **총 부하 비율** | **0.13%** |

### 피크 부하 (5분 Job 동시 시작)

| 항목 | 값 |
|------|-----|
| 동시 시작 Job 수 | 48 jobs |
| 순간 burst 작업 | 86,040 ops |
| 10초 처리 시 | 8,604 ops/sec |
| **피크 부하 비율** | **5.14%** |

### 여유 용량

| 항목 | 값 |
|------|-----|
| 쓰기 여유 | 29,955 ops/sec (99.9%) |
| 총 여유 | 167,280 ops/sec (99.9%) |
| **최대 지원 팀 수** | **~1,869개 팀** |

---

## 🔷 Control Plane 5노드 구성 분석

### etcd 부하 상세

| 부하 항목 | ops/hour |
|-----------|----------|
| Spark 워크로드 (쓰기) | 152,046 |
| Spark 워크로드 (읽기) | 608,184 |
| Node Heartbeat (55노드) | 19,800 |
| 기타 시스템 부하 | 13,380 |
| **총 쓰기** | **162,000** |
| **총 읽기** | **631,410** |
| **총계** | **793,410** |

### 초당 환산

| 메트릭 | ops/sec |
|--------|---------|
| 쓰기 | 45.0 |
| 읽기 | 175.4 |
| **총계** | **220.4** |

### 성능 분석

| 항목 | 값 |
|------|-----|
| 추정 쓰기 처리량 (NVMe) | 25,500 ops/sec (합의 오버헤드로 3노드 대비 85%) |
| 추정 읽기 처리량 | 165,000 ops/sec (분산으로 3노드 대비 120%) |
| 추정 총 처리량 | 190,500 ops/sec |
| 추정 디스크 지연 | 0.5 ms |
| **쓰기 부하 비율** | **0.18%** |
| **총 부하 비율** | **0.12%** |

### 피크 부하

| 항목 | 값 |
|------|-----|
| 동시 시작 Job 수 | 48 jobs |
| 순간 burst 작업 | 86,040 ops |
| 10초 처리 시 | 8,604 ops/sec |
| **피크 부하 비율** | **4.52%** |

### 여유 용량

| 항목 | 값 |
|------|-----|
| 쓰기 여유 | 25,455 ops/sec (99.8%) |
| 총 여유 | 190,280 ops/sec (99.9%) |
| **최대 지원 팀 수** | **~1,586개 팀** |

---

## ⚖️ 3노드 vs 5노드 비교

| 항목 | 3노드 | 5노드 |
|------|-------|-------|
| **쓰기 성능** | 30,000 ops/sec | 25,500 ops/sec |
| **읽기 성능** | 137,500 ops/sec | 165,000 ops/sec |
| **총 성능** | 167,500 ops/sec | 190,500 ops/sec |
| **장애 허용** | 1노드 | 2노드 |
| 쿼럼 | 2/3 | 3/5 |
| 네트워크 트래픽 | 낮음 | 중간 |
| 무중단 유지보수 | 어려움 | 가능 |
| **현재 부하 비율** | **0.13%** | **0.12%** |
| **피크 부하 비율** | **5.14%** | **4.52%** |
| 최대 확장 가능 팀 | ~1,869팀 | ~1,586팀 |

---

## 📦 리소스 사용량 요약

### 동시 실행 Pod

| 항목 | 값 | 기존 대비 |
|------|-----|----------|
| 평균 동시 실행 | 333 pods | 3배 ↑ |
| 피크 시 동시 실행 | 1,344 pods | 3배 ↑ |

### etcd 스토리지

| 항목 | 값 |
|------|-----|
| Spark 워크로드 | 9.1 MB |
| 기본 클러스터 | 50.0 MB |
| **총 사용량** | **59.1 MB** |

> 💡 NVMe 3.84TB × 2 대비 스토리지 사용량은 여전히 무시할 수준 (0.0008%)

---

## 📊 기존 대비 비교 (워크로드 1배 vs 3배)

| 항목 | 1배 (기존) | 3배 (현재) | 증가율 |
|------|-----------|-----------|--------|
| 팀별 워크로드 | 20개 | 60개 | 3배 |
| 시간당 Job 실행 | 282.5 | 847.5 | 3배 |
| 시간당 Pod 수 | 1,626 | 4,878 | 3배 |
| 쓰기 ops/sec | 15.5 | 44.9 | 2.9배 |
| 총 ops/sec | 75.0 | 220.2 | 2.9배 |
| 피크 ops/sec | 2,868 | 8,604 | 3배 |
| **부하 비율 (3노드)** | 0.04% | **0.13%** | 3.3배 |
| **피크 비율 (3노드)** | 1.71% | **5.14%** | 3배 |

---

## 📈 확장 한계 분석

### 70% 활용 기준 최대 지원 가능 팀 수

| 구성 | 현재 (4팀) | 최대 지원 | 확장 여유 |
|------|-----------|----------|----------|
| 3노드 | 4팀 × 60 워크로드 | ~1,869팀 | **467배** |
| 5노드 | 4팀 × 60 워크로드 | ~1,586팀 | **396배** |

### etcd 성능 임계점 도달 시나리오

| 시나리오 | 3노드 도달 조건 | 5노드 도달 조건 |
|----------|----------------|----------------|
| 쓰기 50% | ~668팀 | ~566팀 |
| 쓰기 70% | ~935팀 | ~793팀 |
| 총 부하 50% | ~1,523팀 | ~1,612팀 |
| 총 부하 70% | ~2,132팀 | ~2,257팀 |

---

## 📊 결론 요약

### 핵심 수치 (워크로드 3배 증가 후)

| 항목 | 3노드 | 5노드 |
|------|-------|-------|
| **정상 부하** | 220 ops/sec | 220 ops/sec |
| **피크 부하** | 8,604 ops/sec | 8,604 ops/sec |
| **부하 비율** | 0.13% | 0.12% |
| **피크 비율** | 5.14% | 4.52% |

### 판정

```
✅ 워크로드 3배 증가 후에도 etcd는 병목이 되지 않습니다.

• 정상 부하: etcd 처리 능력의 0.13% 사용
• 피크 부하: etcd 처리 능력의 5.14% 사용
• 99.9% 이상의 여유 용량 확보
• 현재 설정 기준 400배 이상 확장 가능
```
