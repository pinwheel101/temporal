# Apache Flink 2.x vs 1.20 ì¢…í•© ê¸°ìˆ  ë¹„êµ ë¬¸ì„œ

Apache Flink 2.0ì€ **2025ë…„ 3ì›” 24ì¼ ì¶œì‹œ**ëœ 9ë…„ ë§Œì˜ ì²« ë©”ì´ì € ë¦´ë¦¬ìŠ¤ë¡œ,  í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ ì•„í‚¤í…ì²˜ë¡œì˜ ê·¼ë³¸ì  ì „í™˜ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. 165ëª…ì˜ ê¸°ì—¬ìê°€ **25ê°œì˜ FLIP**ê³¼ **369ê°œì˜ ì´ìŠˆ**ë¥¼ ì™„ë£Œí–ˆìœ¼ë©°,  DataSet APIì™€ Scala API ì™„ì „ ì œê±°,   ìƒˆë¡œìš´ ë¶„ë¦¬í˜• ìƒíƒœ ê´€ë¦¬(ForSt), ë¹„ë™ê¸° ì‹¤í–‰ ëª¨ë¸ ë„ì…ì´ í•µì‹¬ ë³€í™”ì…ë‹ˆë‹¤.  **1.xì™€ 2.x ê°„ ìƒíƒœ í˜¸í™˜ì„±ì€ ë³´ì¥ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ**   ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œ ì‹ ê·œ Savepoint ê³„íšì´ í•„ìˆ˜ì…ë‹ˆë‹¤.

-----

## 1. ì•„í‚¤í…ì²˜ ë³€ê²½ì‚¬í•­

### ForSt State Backend: í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ ìƒíƒœ ê´€ë¦¬ì˜ í•µì‹¬

**ForSt (For Streaming)**ëŠ” Flink 2.0ì˜ ê°€ì¥ í˜ì‹ ì ì¸ ì•„í‚¤í…ì²˜ ë³€ê²½ìœ¼ë¡œ, ì»´í“¨íŠ¸ì™€ ìŠ¤í† ë¦¬ì§€ë¥¼ ì™„ì „íˆ ë¶„ë¦¬í•œ ë¶„ì‚° ìƒíƒœ ë°±ì—”ë“œì…ë‹ˆë‹¤.  ê¸°ì¡´ RocksDBê°€ ë¡œì»¬ ë””ìŠ¤í¬ì— ìƒíƒœë¥¼ ì €ì¥í–ˆë‹¤ë©´, ForStëŠ” **S3, HDFS ë“± ì›ê²© DFSë¥¼ ì£¼ ì €ì¥ì†Œ**ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.  

|íŠ¹ì„±          |RocksDB (1.20)|ForSt (2.0)       |
|------------|--------------|------------------|
|**ì£¼ ì €ì¥ì†Œ**   |ë¡œì»¬ ë””ìŠ¤í¬        |ì›ê²© DFS (S3/HDFS)  |
|**ìƒíƒœ í¬ê¸° ì œí•œ**|ë¡œì»¬ ë””ìŠ¤í¬ ìš©ëŸ‰     |**ë¬´ì œí•œ**           |
|**ì²´í¬í¬ì¸íŠ¸ ë°©ì‹**|ì „ì²´ ìƒíƒœ ì—…ë¡œë“œ     |ë©”íƒ€ë°ì´í„°ë§Œ (Zero-copy)|
|**ë³µêµ¬ ì‹œê°„**   |ìƒíƒœ í¬ê¸°ì— ë¹„ë¡€     |ìƒíƒœ í¬ê¸°ì™€ **ë¬´ê´€**     |
|**ë¦¬ì†ŒìŠ¤ ì‚¬ìš©**  |ì£¼ê¸°ì  ìŠ¤íŒŒì´í¬      |ì•ˆì •ì /ì§€ì†ì            |
|**ë¦¬ìŠ¤ì¼€ì¼ë§**   |ëŠë¦¼ (ìƒíƒœ ì¬ë¶„ë°°)   |ë¹ ë¦„ (íŒŒì¼ ì°¸ì¡°ë§Œ ë³€ê²½)    |

ForStì˜ ì•„í‚¤í…ì²˜ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Task Manager                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚      ForSt Instance                 â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚ Memory Blockâ”‚  â”‚ Local Disk   â”‚  â”‚ â”‚
â”‚  â”‚  â”‚   Cache     â”‚  â”‚   Cache      â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    DFS (S3/HDFS/OSS/GFS)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Working Directory (SST Files)     â”‚ â”‚
â”‚  â”‚   Checkpoint Directory              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Nexmark ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼**ì—ì„œ I/O ì§‘ì•½ì  ì¿¼ë¦¬ì˜ ê²½ìš° 1GB ìºì‹œë§Œìœ¼ë¡œ ë¡œì»¬ ìŠ¤í† ì–´ ëŒ€ë¹„ **75%~120% ì²˜ë¦¬ëŸ‰**ì„ ë‹¬ì„±í–ˆìœ¼ë©°,  HDFS ë¹„ë™ê¸° ëª¨ë“œëŠ” ë™ê¸° ëª¨ë“œ ëŒ€ë¹„ ì•½ **2ë°° ì²˜ë¦¬ëŸ‰ í–¥ìƒ**ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. 

### ë¹„ë™ê¸° ì‹¤í–‰ ëª¨ë¸ì˜ ë„ì…

Flink 2.0ì˜ ë˜ ë‹¤ë¥¸ í•µì‹¬ ë³€í™”ëŠ” **ë¹„ë™ê¸° ì‹¤í–‰ ëª¨ë¸(Asynchronous Execution Model)**ì…ë‹ˆë‹¤.   ê¸°ì¡´ ë™ê¸°ì‹ ëª¨ë¸ì—ì„œëŠ” ìƒíƒœ ì½ê¸° ì‹œ ë©”ì¸ íƒœìŠ¤í¬ ìŠ¤ë ˆë“œê°€ ë¸”ë¡œí‚¹ë˜ì–´ HDFS ì ‘ê·¼ ì§€ì—°(1.5ms)ì´ ë¡œì»¬ ë””ìŠ¤í¬(68Î¼s)ë³´ë‹¤ 20ë°° ì´ìƒ ë†’ì•˜ê³ , DFS ì‚¬ìš© ì‹œ TPSê°€ 95% ê°ì†Œí•˜ëŠ” ë¬¸ì œê°€ ìˆì—ˆìŠµë‹ˆë‹¤.

ìƒˆë¡œìš´ ë¹„ë™ê¸° ëª¨ë¸ì€ ìƒíƒœ ì ‘ê·¼ê³¼ ê³„ì‚°ì„ ë¶„ë¦¬í•˜ì—¬ ë³‘ë ¬ ì‹¤í–‰í•˜ë©°,  ë‹¤ìŒ ì„¸ ê°€ì§€ í•µì‹¬ ë³´ì¥ì„ ìœ ì§€í•©ë‹ˆë‹¤:

- **ë™ì¼ í‚¤ ë ˆì½”ë“œ ì²˜ë¦¬ ìˆœì„œ ë³´ì¥**
- **ì²´í¬í¬ì¸íŠ¸ ë™ê¸°í™” ê´€ë¦¬**
- **ì›Œí„°ë§ˆí¬/íƒ€ì´ë¨¸ ì‹œë§¨í‹± ìœ ì§€** 

### Checkpoint/Savepoint ì•„í‚¤í…ì²˜ ê°œì„ 

ForSt ë°±ì—”ë“œì—ì„œëŠ” Working Directoryì™€ Checkpoint Directoryê°€ ë™ì¼í•œ DFSë¥¼ ê³µìœ í•˜ì—¬ **Zero-Copy ì²´í¬í¬ì¸íŠ¸**ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤:  

```
ê¸°ì¡´ (1.20):
1. ìƒíƒœ â†’ ë¡œì»¬ ë””ìŠ¤í¬
2. ì²´í¬í¬ì¸íŠ¸ íŠ¸ë¦¬ê±° â†’ ì „ì²´ ìƒíƒœ DFS ì—…ë¡œë“œ
3. ì—…ë¡œë“œ ì™„ë£Œ â†’ ì²´í¬í¬ì¸íŠ¸ ì™„ë£Œ

Flink 2.0:
1. ìƒíƒœ â†’ DFS (ì§€ì†ì  ìŠ¤íŠ¸ë¦¬ë°)
2. ì²´í¬í¬ì¸íŠ¸ íŠ¸ë¦¬ê±° â†’ ë©”íƒ€ë°ì´í„°ë§Œ ì €ì¥
3. íŒŒì¼ ì°¸ì¡° ê³µìœ  â†’ ê±°ì˜ ì¦‰ì‹œ ì™„ë£Œ
```

Flink 2.0ì—ì„œ **ë„¤ì´í‹°ë¸Œ savepoint í¬ë§·ì´ ê¸°ë³¸ê°’**ìœ¼ë¡œ ë³€ê²½ë˜ë©°, **LEGACY ë³µì› ëª¨ë“œëŠ” ì œê±°**ë˜ì—ˆìŠµë‹ˆë‹¤. 

-----

## 2. Java API ë ˆë²¨ ë³€ê²½ì‚¬í•­

### ì™„ì „íˆ ì œê±°ëœ API ì„¸íŠ¸

|ì œê±°ëœ API                         |ëŒ€ì²´ API                              |ì˜í–¥ë„ |
|--------------------------------|------------------------------------|----|
|**DataSet API**                 |DataStream API ë˜ëŠ” Table API/SQL     |ğŸ”´ ë†’ìŒ|
|**Scala DataStream/DataSet API**|Java DataStream API                 |ğŸ”´ ë†’ìŒ|
|**SourceFunction, SinkFunction**|Source V2, Sink V2                  |ğŸ”´ ë†’ìŒ|
|**TableSource, TableSink**      |DynamicTableSource, DynamicTableSink|ğŸ”´ ë†’ìŒ|
|**flink-conf.yaml**             |config.yaml (í‘œì¤€ YAML)               |ğŸŸ¡ ì¤‘ê°„|
|**Java 8 ì§€ì›**                   |Java 11+ (ê¶Œì¥ Java 17)               |ğŸ”´ ë†’ìŒ|

### Source API ë§ˆì´ê·¸ë ˆì´ì…˜ (FLIP-27)

```java
// âŒ Flink 1.20 - SourceFunction (ì œê±°ë¨)
env.addSource(new FlinkKafkaConsumer<>(...));

// âœ… Flink 2.x - FLIP-27 Source API
KafkaSource<String> source = KafkaSource.<String>builder()
    .setBootstrapServers("localhost:9092")
    .setTopics("input-topic")
    .setGroupId("my-group")
    .setStartingOffsets(OffsetsInitializer.earliest())
    .setValueOnlyDeserializer(new SimpleStringSchema())
    .build();

env.fromSource(source, WatermarkStrategy.noWatermarks(), "Kafka Source");
```

ìƒˆë¡œìš´ Source APIì˜ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ëŠ” **Source**(íŒ©í† ë¦¬), **SplitEnumerator**(ë¶„í•  ë°œê²¬/í• ë‹¹), **SourceReader**(ì‹¤ì œ ì½ê¸°), **SourceSplit**(ì‘ì—… ë‹¨ìœ„)ì…ë‹ˆë‹¤. 

### Sink API ë§ˆì´ê·¸ë ˆì´ì…˜ (FLIP-143/191)

```java
// âŒ Flink 1.20 - SinkFunction (ì œê±°ë¨)
stream.addSink(new FlinkKafkaProducer<>(...));

// âœ… Flink 2.x - Sink V2 API
KafkaSink<String> sink = KafkaSink.<String>builder()
    .setBootstrapServers("localhost:9092")
    .setRecordSerializer(KafkaRecordSerializationSchema.builder()
        .setTopic("output-topic")
        .setValueSerializationSchema(new SimpleStringSchema())
        .build())
    .setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE)
    .build();

stream.sinkTo(sink);
```

### RichFunction.open() ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ë³€ê²½

```java
// âŒ Flink 1.20
@Override
public void open(Configuration parameters) throws Exception {
    // ì´ˆê¸°í™” ë¡œì§
}

// âœ… Flink 2.x
@Override
public void open(OpenContext openContext) throws Exception {
    // ì´ˆê¸°í™” ë¡œì§
}
```

### ë¹„ë™ê¸° State API (State API V2)

Flink 2.0ì—ì„œ ë¹„ë™ê¸° ìƒíƒœ ì ‘ê·¼ APIê°€ ë„ì…ë˜ì–´ ë…¼ë¸”ë¡œí‚¹ ìƒíƒœ ì ‘ê·¼ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤:  

```java
// âŒ ê¸°ì¡´ ë™ê¸°ì‹ API (1.20)
Integer val = wordCounter.value();  // ë¸”ë¡œí‚¹
int updated = (val == null ? 1 : val + 1);
wordCounter.update(updated);

// âœ… ìƒˆë¡œìš´ ë¹„ë™ê¸° API (2.0)
wordCounter.asyncValue()
    .thenCompose(val -> {
        int updated = (val == null ? 1 : val + 1);
        return wordCounter.asyncUpdate(updated);
    })
    .thenAccept(empty -> {
        out.collect(Tuple2.of(value.f0, updated.get()));
    });
```

### State TTL ë³€ê²½ì‚¬í•­

```java
// âŒ Flink 1.20 - Time ì‚¬ìš©
import org.apache.flink.api.common.time.Time;
StateTtlConfig.newBuilder(Time.seconds(10))

// âœ… Flink 2.x - Duration ì‚¬ìš©
import java.time.Duration;
StateTtlConfig ttlConfig = StateTtlConfig
    .newBuilder(Duration.ofMinutes(10))
    .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)
    .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)
    .cleanupFullSnapshot()
    .cleanupInRocksdbCompactFilter(1000, Duration.ofDays(30))
    .build();
```

### keyBy ë©”ì„œë“œ ë³€ê²½

```java
// âŒ Flink 1.20 (ì œê±°ë¨)
stream.keyBy(0)              // í•„ë“œ ì¸ë±ìŠ¤ ì‚¬ìš©
stream.keyBy("fieldName")    // í•„ë“œ ì´ë¦„ ì‚¬ìš©

// âœ… Flink 2.x (ê¶Œì¥)
stream.keyBy(value -> value.f0)     // KeySelector ì‚¬ìš©
stream.keyBy(MyClass::getKey)       // ë©”ì„œë“œ ì°¸ì¡° ì‚¬ìš©
```

### Table API / SQL API ë³€ê²½

**ìƒˆë¡œìš´ SQL ê¸°ëŠ¥:**

```sql
-- C-style Escape ë¬¸ìì—´
SELECT E'Hello\nWorld';

-- QUALIFY ì ˆ (ìœˆë„ìš° í•¨ìˆ˜ í•„í„°ë§)
SELECT * FROM orders
QUALIFY ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY order_time DESC) = 1;

-- TABLE() ë˜í¼ ì—†ì´ í…Œì´ë¸” í•¨ìˆ˜ í˜¸ì¶œ
SELECT * FROM TUMBLE(orders, DESCRIPTOR(order_time), INTERVAL '1' HOUR);
```

-----

## 3. Breaking Changes ìƒì„¸ ëª©ë¡

### ì œê±°ëœ ì£¼ìš” í´ë˜ìŠ¤

**Core API:**

- `org.apache.flink.api.common.ExecutionMode`
- `org.apache.flink.api.common.time.Time` â†’ `java.time.Duration`
- `org.apache.flink.api.common.restartstrategy.RestartStrategies` ì „ì²´

**Sink ê´€ë ¨:**

- `SinkFunction`, `RichSinkFunction`, `PrintSinkFunction`
- `TwoPhaseCommitSinkFunction`, `StreamingFileSink`

**Source ê´€ë ¨:**

- `SourceFunction`, `RichSourceFunction`, `ParallelSourceFunction`
- `FromElementsFunction`, `SocketTextStreamFunction`

**State Backend:**

- `FsStateBackend`, `MemoryStateBackend` (HashMapStateBackend + FileSystemCheckpointStorageë¡œ ëŒ€ì²´)

**DataSet API ì „ì²´ íŒ¨í‚¤ì§€:**

- `org.apache.flink.api.java.DataSet`
- `org.apache.flink.api.java.ExecutionEnvironment`
- ëª¨ë“  DataSet ì—°ì‚°ì ë° I/O í´ë˜ìŠ¤

### ì œê±°ëœ ì„¤ì • ì˜µì…˜ (ì£¼ìš” í•­ëª©)

```yaml
# CheckpointingOptions
checkpointing.local-recovery â†’ ì œê±°ë¨
state.backend â†’ ì œê±°ë¨
state.backend.async â†’ ì œê±°ë¨

# JobManagerOptions
jobmanager.heap.size â†’ ì œê±°ë¨
jobmanager.scheduler: "Ng" â†’ ì œê±°ë¨
jobmanager.speculative.enabled â†’ ì œê±°ë¨

# NetworkOptions
taskmanager.network.blocking-shuffle.* â†’ ì œê±°ë¨
taskmanager.network.hybrid-shuffle.enable-new-mode â†’ ì œê±°ë¨

# TableOptions
table.exec.legacy-transformation-uids â†’ ì œê±°ë¨
table.exec.shuffle-mode â†’ ì œê±°ë¨
```

### ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ë³€ê²½

```java
// ExecutionConfig - ì œê±°ëœ ë©”ì„œë“œë“¤
void setRestartStrategy(RestartStrategyConfiguration)  // ì œê±°ë¨
void setExecutionMode(ExecutionMode)                   // ì œê±°ë¨

// TypeInformation
TypeSerializer<T> createSerializer(ExecutionConfig)    // ì œê±°ë¨
// â†’ TypeSerializer<T> createSerializer(SerializerConfig) ì‚¬ìš©

// OutputFormat
void open(int taskNumber, int numTasks)                // ì œê±°ë¨
// â†’ void open(OutputFormat.InitializationContext context) ì‚¬ìš©
```

### ì»¤ë„¥í„° í˜¸í™˜ì„± í˜„í™©

|ì»¤ë„¥í„°              |Flink 2.0 í˜¸í™˜ ë²„ì „              |ìƒíƒœ    |
|-----------------|-----------------------------|------|
|**Kafka**        |`flink-connector-kafka:4.0.1`|âœ… ì¶œì‹œë¨ |
|**JDBC**         |`flink-connector-jdbc:4.0.0` |âœ… ì¶œì‹œë¨ |
|**Elasticsearch**|Flink 2.0 í˜¸í™˜ ë²„ì „              |âœ… ì¶œì‹œë¨ |
|**Paimon**       |Flink 2.0 í˜¸í™˜ ë²„ì „              |âœ… ì¶œì‹œë¨ |
|**ê¸°íƒ€**           |Flink 2.3ê¹Œì§€ ìˆœì°¨ ì§€ì›            |ğŸ”„ ì§„í–‰ ì¤‘|

### Java ë²„ì „ ë³€ê²½

|ë²„ì „     |Flink 1.20|Flink 2.x  |
|-------|----------|-----------|
|Java 8 |âœ… ì§€ì›      |âŒ **ì œê±°**   |
|Java 11|âœ… ì§€ì›      |âœ… ìµœì†Œ ë²„ì „    |
|Java 17|âœ… ì§€ì›      |âœ… **ê¸°ë³¸/ê¶Œì¥**|
|Java 21|âŒ ë¯¸ì§€ì›     |âœ… ê³µì‹ ì§€ì›    |

-----

## 4. Flink 2.x ì‹ ê·œ ê¸°ëŠ¥

### Materialized Table

**Materialized Table**ì€ ë°°ì¹˜ì™€ ìŠ¤íŠ¸ë¦¼ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ í†µí•©í•˜ëŠ” ìƒˆë¡œìš´ í…Œì´ë¸” ìœ í˜•ì…ë‹ˆë‹¤. 

```sql
-- Materialized Table ìƒì„±
CREATE MATERIALIZED TABLE my_materialized_table
PARTITIONED BY (ds)
WITH (
    'format' = 'json',
    'partition.fields.ds.date-formatter' = 'yyyy-MM-dd'
)
FRESHNESS = INTERVAL '1' HOUR
AS SELECT 
    user_id,
    COUNT(*) as order_count,
    SUM(amount) as total_amount,
    ds
FROM orders
GROUP BY user_id, ds;

-- í…Œì´ë¸” ê´€ë¦¬
ALTER MATERIALIZED TABLE my_table REFRESH;                    -- ìˆ˜ë™ ìƒˆë¡œê³ ì¹¨
ALTER MATERIALIZED TABLE my_table SUSPEND;                    -- ì¼ì‹œ ì¤‘ì§€
ALTER MATERIALIZED TABLE my_table RESUME;                     -- ì¬ê°œ
SHOW MATERIALIZED TABLES;                                      -- ëª©ë¡ ì¡°íšŒ (2.2+)
```

**ìƒˆë¡œê³ ì¹¨ ëª¨ë“œ:**

- `CONTINUOUS`: ìŠ¤íŠ¸ë¦¬ë° ì‘ì—…ì´ ì§€ì†ì ìœ¼ë¡œ ë°ì´í„° ê°±ì‹ 
- `FULL`: ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì£¼ê¸°ì ìœ¼ë¡œ ë°°ì¹˜ ì‘ì—… íŠ¸ë¦¬ê±° 

### í–¥ìƒëœ Watermark ì •ë ¬

Split ë ˆë²¨ê¹Œì§€ í™•ì¥ëœ Watermark ì •ë ¬ë¡œ ë¶ˆê· í˜• ì†ŒìŠ¤ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤:  

```java
DataStream<Long> eventStream = env.fromSource(
    new NumberSequenceSource(0, Long.MAX_VALUE),
    WatermarkStrategy.<Long>forMonotonousTimestamps()
        .withTimestampAssigner(new LongTimestampAssigner())
        .withWatermarkAlignment(
            "alignment-group-1",      // ì •ë ¬ ê·¸ë£¹ ë ˆì´ë¸”
            Duration.ofSeconds(30),   // ìµœëŒ€ í—ˆìš© ë“œë¦¬í”„íŠ¸
            Duration.ofSeconds(1)     // ì—…ë°ì´íŠ¸ ê°„ê²©
        ),
    "NumberSequenceSource"
);
```

### Adaptive Batch Execution ê°œì„ 

**10TB TPC-DS ë²¤ì¹˜ë§ˆí¬** ê²°ê³¼:

- ANALYZE TABLE í†µê³„ ì •ë³´ ì‚¬ìš© ì‹œ: Flink 1.20 ëŒ€ë¹„ **8% ì„±ëŠ¥ í–¥ìƒ**
- ì¶”ê°€ í†µê³„ ì •ë³´ ì—†ì´: Flink 1.20 ëŒ€ë¹„ **16% ì„±ëŠ¥ í–¥ìƒ** 

ì£¼ìš” ê°œì„  ì‚¬í•­:

- **Adaptive Broadcast Join**: ëŸ°íƒ€ì„ì— ì…ë ¥ í¬ê¸° ê¸°ë°˜ ìë™ ì „í™˜ 
- **Automatic Join Skew Optimization**: ìŠ¤íëœ ë°ì´í„° íŒŒí‹°ì…˜ ë™ì  ë¶„í•   

### AI í†µí•© ê¸°ëŠ¥ (Flink 2.1/2.2)

```sql
-- ML_PREDICT í•¨ìˆ˜ (2.1+)
SELECT ML_PREDICT('openai-model', text_column) FROM logs;

-- VECTOR_SEARCH í•¨ìˆ˜ (2.2)
SELECT VECTOR_SEARCH(embedding_column, 'similarity_index') FROM data;
```

### ì§ë ¬í™” ê°œì„ 

- **ì»¬ë ‰ì…˜ íƒ€ì… ì§ë ¬í™”ê¸°**: Map/List/Setì— ëŒ€í•œ íš¨ìœ¨ì ì¸ ë‚´ì¥ ì§ë ¬í™”ê¸° (ê¸°ë³¸ í™œì„±í™”) 
- **Kryo 5.6 ì—…ê·¸ë ˆì´ë“œ**: ë” ë¹ ë¥´ê³  ë©”ëª¨ë¦¬ íš¨ìœ¨ì , ìµœì‹  Java ë²„ì „ ì§€ì› ê°œì„   

-----

## 5. ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ

### ê¶Œì¥ ë§ˆì´ê·¸ë ˆì´ì…˜ ìˆœì„œ

1. **Java ë²„ì „ ì—…ê·¸ë ˆì´ë“œ**: Java 8 â†’ Java 11/17/21 (ê¶Œì¥: Java 17)
1. **ì„¤ì • íŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜**: `flink-conf.yaml` â†’ `config.yaml`
1. **Deprecated API ì œê±° í™•ì¸ ë° ì½”ë“œ ìˆ˜ì •**
1. **ì»¤ë„¥í„° ë²„ì „ ì—…ê·¸ë ˆì´ë“œ**
1. **ìƒˆ í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸**
1. **ìƒˆ Savepoint ìƒì„± í›„ ë°°í¬**

### ì„¤ì • íŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜

```bash
# ë§ˆì´ê·¸ë ˆì´ì…˜ ë„êµ¬ ì‚¬ìš©
bin/flink migrate-config --source flink-conf.yaml --target config.yaml
```

**ìƒˆë¡œìš´ config.yaml í˜•ì‹:**

```yaml
jobmanager:
  rpc:
    address: localhost
    port: 6123
  memory:
    process:
      size: 1600m

taskmanager:
  memory:
    process:
      size: 1728m
  numberOfTaskSlots: 1

parallelism:
  default: 1
```

### ì½”ë“œ ë§ˆì´ê·¸ë ˆì´ì…˜ íŒ¨í„´

**ExecutionConfig API ë³€ê²½:**

```java
// âŒ Flink 1.20
env.getConfig().setRestartStrategy(
    RestartStrategies.fixedDelayRestart(3, Time.seconds(10)));

// âœ… Flink 2.x
Configuration config = new Configuration();
config.set(RestartStrategyOptions.RESTART_STRATEGY, "fixed-delay");
config.set(RestartStrategyOptions.RESTART_STRATEGY_FIXED_DELAY_ATTEMPTS, 3);
config.set(RestartStrategyOptions.RESTART_STRATEGY_FIXED_DELAY_DELAY, 
    Duration.ofSeconds(10));
env.configure(config);
```

### ìƒíƒœ í˜¸í™˜ì„± ì£¼ì˜ì‚¬í•­

**í•µì‹¬ ì£¼ì˜:** 1.x â†’ 2.x ê°„ ìƒíƒœ í˜¸í™˜ì„±ì´ **ë³´ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤**.  

ê¶Œì¥ ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ:

1. 1.20ì—ì„œ ìµœì¢… Savepoint ìƒì„±
1. 2.x í™˜ê²½ì—ì„œ ìƒˆ Job ë°°í¬ (ìƒíƒœ ì—†ì´ ì‹œì‘)
1. ë°ì´í„° ì¬ì²˜ë¦¬ ë˜ëŠ” ì™¸ë¶€ ìƒíƒœ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜ í™œìš©

-----

## 6. Flink 2.x ê°œë°œ ì‹œì‘ ê°€ì´ë“œ

### Maven í”„ë¡œì íŠ¸ ì„¤ì •

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 
         http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    
    <groupId>com.example</groupId>
    <artifactId>flink-job</artifactId>
    <version>1.0-SNAPSHOT</version>
    
    <properties>
        <flink.version>2.0.0</flink.version>
        <java.version>17</java.version>
        <maven.compiler.source>${java.version}</maven.compiler.source>
        <maven.compiler.target>${java.version}</maven.compiler.target>
    </properties>
    
    <dependencies>
        <!-- Core Streaming API -->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-streaming-java</artifactId>
            <version>${flink.version}</version>
            <scope>provided</scope>
        </dependency>
        
        <!-- Clients -->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-clients</artifactId>
            <version>${flink.version}</version>
            <scope>provided</scope>
        </dependency> [![](claude-citation:/icon.png?validation=C9FA770F-2139-48CD-8E05-774CDC5D1D07&citation=eyJlbmRJbmRleCI6MTI5MjEsIm1ldGFkYXRhIjp7Imljb25VcmwiOiJodHRwczpcL1wvd3d3Lmdvb2dsZS5jb21cL3MyXC9mYXZpY29ucz9zej02NCZkb21haW49YXBhY2hlLm9yZyIsInByZXZpZXdUaXRsZSI6IkFwYWNoZSBGbGluayAyLjAuMSBSZWxlYXNlIEFubm91bmNlbWVudCB8IEFwYWNoZSBGbGluayIsInNvdXJjZSI6IkFwYWNoZSBGbGluayIsInR5cGUiOiJnZW5lcmljX21ldGFkYXRhIn0sInNvdXJjZXMiOlt7Imljb25VcmwiOiJodHRwczpcL1wvd3d3Lmdvb2dsZS5jb21cL3MyXC9mYXZpY29ucz9zej02NCZkb21haW49YXBhY2hlLm9yZyIsInNvdXJjZSI6IkFwYWNoZSBGbGluayIsInRpdGxlIjoiQXBhY2hlIEZsaW5rIDIuMC4xIFJlbGVhc2UgQW5ub3VuY2VtZW50IHwgQXBhY2hlIEZsaW5rIiwidXJsIjoiaHR0cHM6XC9cL2ZsaW5rLmFwYWNoZS5vcmdcLzIwMjVcLzExXC8xMFwvYXBhY2hlLWZsaW5rLTIuMC4xLXJlbGVhc2UtYW5ub3VuY2VtZW50XC8ifV0sInN0YXJ0SW5kZXgiOjExNjY5LCJ0aXRsZSI6IkFwYWNoZSBGbGluayIsInVybCI6Imh0dHBzOlwvXC9mbGluay5hcGFjaGUub3JnXC8yMDI1XC8xMVwvMTBcL2FwYWNoZS1mbGluay0yLjAuMS1yZWxlYXNlLWFubm91bmNlbWVudFwvIiwidXVpZCI6ImJlYjE3ODlmLTVhYTktNGI4NC1hMTgzLTAzYzg1M2Y0MTAwMSJ9 "Apache Flink")](https://flink.apache.org/2025/11/10/apache-flink-2.0.1-release-announcement/)
        
        <!-- Kafka Connector -->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-connector-kafka</artifactId>
            <version>4.0.1</version>
        </dependency>
        
        <!-- Test -->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-test-utils</artifactId>
            <version>${flink.version}</version>
            <scope>test</scope>
        </dependency>
    </dependencies>
</project>
```

**Artifact ID ë³€ê²½ ì£¼ì˜:** Scala suffixê°€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.

- `flink-streaming-java_2.12` â†’ `flink-streaming-java`
- `flink-clients_2.12` â†’ `flink-clients`

### ê¸°ë³¸ DataStream ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì¡°

```java
import org.apache.flink.api.common.functions.FilterFunction;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;

public class DataStreamJob {
    public static void main(String[] args) throws Exception {
        // 1. ì‹¤í–‰ í™˜ê²½ ìƒì„±
        final StreamExecutionEnvironment env = 
            StreamExecutionEnvironment.getExecutionEnvironment();
        
        // 2. ë°ì´í„° ì†ŒìŠ¤ ì •ì˜ (FLIP-27 Source API)
        KafkaSource<String> source = KafkaSource.<String>builder()
            .setBootstrapServers("localhost:9092")
            .setTopics("input-topic")
            .setGroupId("flink-group")
            .setStartingOffsets(OffsetsInitializer.earliest())
            .setValueOnlyDeserializer(new SimpleStringSchema())
            .build();
        
        DataStream<String> stream = env.fromSource(
            source, WatermarkStrategy.noWatermarks(), "Kafka Source");
        
        // 3. ë³€í™˜ ì ìš©
        DataStream<String> processed = stream
            .filter((FilterFunction<String>) value -> value.length() > 4)
            .map((MapFunction<String, String>) String::toUpperCase);
        
        // 4. ì‹±í¬ ì •ì˜ (Sink V2 API)
        KafkaSink<String> sink = KafkaSink.<String>builder()
            .setBootstrapServers("localhost:9092")
            .setRecordSerializer(KafkaRecordSerializationSchema.builder()
                .setTopic("output-topic")
                .setValueSerializationSchema(new SimpleStringSchema())
                .build())
            .setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE)
            .build();
        
        processed.sinkTo(sink);
        
        // 5. ì‹¤í–‰
        env.execute("Flink 2.x DataStream Job");
    }
}
```

### ìƒíƒœ ê´€ë¦¬ ëª¨ë²” ì‚¬ë¡€

```java
public class StatefulFunction extends KeyedProcessFunction<String, Event, Result> {
    
    private ValueState<Long> countState;
    private MapState<String, Long> mapState;
    
    @Override
    public void open(OpenContext openContext) throws Exception {
        // TTL ì„¤ì •
        StateTtlConfig ttlConfig = StateTtlConfig
            .newBuilder(Duration.ofHours(1))
            .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)
            .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)
            .cleanupFullSnapshot()
            .build();
        
        ValueStateDescriptor<Long> countDescriptor = 
            new ValueStateDescriptor<>("count", Long.class);
        countDescriptor.enableTimeToLive(ttlConfig);
        countState = getRuntimeContext().getState(countDescriptor);
        
        MapStateDescriptor<String, Long> mapDescriptor = 
            new MapStateDescriptor<>("map-state", String.class, Long.class);
        mapState = getRuntimeContext().getMapState(mapDescriptor);
    }
    
    @Override
    public void processElement(Event event, Context ctx, Collector<Result> out) 
            throws Exception {
        Long currentCount = countState.value();
        if (currentCount == null) {
            currentCount = 0L;
        }
        currentCount++;
        countState.update(currentCount);
        
        out.collect(new Result(event.getKey(), currentCount));
    }
}
```

### ForSt State Backend ì„¤ì •

```yaml
# config.yaml
state.backend.type: forst
table.exec.async-state.enabled: true
execution.checkpointing.incremental: true
execution.checkpointing.dir: s3://your-bucket/flink-checkpoints

# ì•„ì§ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê¸° [![](claude-citation:/icon.png?validation=C9FA770F-2139-48CD-8E05-774CDC5D1D07&citation=eyJlbmRJbmRleCI6MTcxNTQsIm1ldGFkYXRhIjp7Imljb25VcmwiOiJodHRwczpcL1wvd3d3Lmdvb2dsZS5jb21cL3MyXC9mYXZpY29ucz9zej02NCZkb21haW49YXBhY2hlLm9yZyIsInByZXZpZXdUaXRsZSI6IkRpc2FnZ3JlZ2F0ZWQgU3RhdGUgTWFuYWdlbWVudCB8IEFwYWNoZSBGbGluayIsInNvdXJjZSI6IkFwYWNoZSIsInR5cGUiOiJnZW5lcmljX21ldGFkYXRhIn0sInNvdXJjZXMiOlt7Imljb25VcmwiOiJodHRwczpcL1wvd3d3Lmdvb2dsZS5jb21cL3MyXC9mYXZpY29ucz9zej02NCZkb21haW49YXBhY2hlLm9yZyIsInNvdXJjZSI6IkFwYWNoZSIsInRpdGxlIjoiRGlzYWdncmVnYXRlZCBTdGF0ZSBNYW5hZ2VtZW50IHwgQXBhY2hlIEZsaW5rIiwidXJsIjoiaHR0cHM6XC9cL25pZ2h0bGllcy5hcGFjaGUub3JnXC9mbGlua1wvZmxpbmstZG9jcy1tYXN0ZXJcL2RvY3NcL29wc1wvc3RhdGVcL2Rpc2FnZ3JlZ2F0ZWRfc3RhdGVcLyJ9XSwic3RhcnRJbmRleCI6MTY5NDgsInRpdGxlIjoiQXBhY2hlIiwidXJsIjoiaHR0cHM6XC9cL25pZ2h0bGllcy5hcGFjaGUub3JnXC9mbGlua1wvZmxpbmstZG9jcy1tYXN0ZXJcL2RvY3NcL29wc1wvc3RhdGVcL2Rpc2FnZ3JlZ2F0ZWRfc3RhdGVcLyIsInV1aWQiOiJhOWE3NGU0Ni1hYzgyLTQzZmUtOTg3Yi0zYTczMTZkNTJiYWEifQ%3D%3D "Apache")](https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/disaggregated_state/)ëŠ¥ ë¹„í™œì„±í™” (ForSt ì‹¤í—˜ì  ë‹¨ê³„)
table.exec.mini-batch.enabled: false
table.optimizer.agg-phase-strategy: ONE_PHASE
```

### Kubernetes ë°°í¬ ì„¤ì •

```yaml
apiVersion: flink.apache.org/v1beta1
kind: FlinkDeployment
metadata:
  name: flink-job
  namespace: flink
spec:
  image: flink:2.0.0
  flinkVersion: v2_0
  flinkConfiguration:
    taskmanager.numberOfTaskSlots: "2"
    state.backend.type: rocksdb
    state.checkpoints.dir: s3://bucket/checkpoints
    state.savepoints.dir: s3://bucket/savepoints
    execution.checkpointing.interval: "60000"
    execution.checkpointing.mode: EXACTLY_ONCE
    restart-strategy.type: exponential-delay
    restart-strategy.exponential-delay.initial-backoff: 10s
    restart-strategy.exponential-delay.max-backoff: 5min
  serviceAccount: flink
  jobManager:
    resource:
      memory: "2048m"
      cpu: 1
  taskManager:
    resource:
      memory: "4096m"
      cpu: 2
    replicas: 3
  job:
    jarURI: local:///opt/flink/usrlib/my-job.jar
    parallelism: 6
    upgradeMode: savepoint
    state: running
```

### í…ŒìŠ¤íŠ¸ ì „ëµ

```java
import org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration;
import org.apache.flink.test.util.MiniClusterWithClientResource;
import org.junit.ClassRule;
import org.junit.Test;

public class StreamingJobIntegrationTest {
    
    @ClassRule
    public static MiniClusterWithClientResource flinkCluster =
        new MiniClusterWithClientResource(
            new MiniClusterResourceConfiguration.Builder()
                .setNumberSlotsPerTaskManager(2)
                .setNumberTaskManagers(1)
                .build());
    
    @Test
    public void testStreamingJob() throws Exception {
        StreamExecutionEnvironment env = 
            StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(2);
        
        // í…ŒìŠ¤íŠ¸ ë¡œì§
        env.fromData(1L, 2L, 3L)
            .map(new IncrementMapFunction())
            .print();
        
        env.execute();
    }
}
```

-----

## ê²°ë¡ 

Apache Flink 2.0ì€ í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ í™˜ê²½ì„ ìœ„í•œ **ê·¼ë³¸ì ì¸ ì•„í‚¤í…ì²˜ ì „í™˜**ì„ ì œê³µí•©ë‹ˆë‹¤. **ForSt State Backend**ì™€ **ë¹„ë™ê¸° ì‹¤í–‰ ëª¨ë¸**ì€ ëŒ€ê·œëª¨ ìƒíƒœ ì²˜ë¦¬ì˜ ë³‘ëª©ì„ í•´ê²°í•˜ê³ , **Materialized Table**ì€ ìŠ¤íŠ¸ë¦¼-ë°°ì¹˜ í†µí•©ì„ ë‹¨ìˆœí™”í•©ë‹ˆë‹¤.

ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œ ê°€ì¥ ì¤‘ìš”í•œ ì ì€ **ìƒíƒœ í˜¸í™˜ì„±ì´ ë³´ì¥ë˜ì§€ ì•ŠëŠ”ë‹¤**ëŠ” ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì‹ ì¤‘í•œ ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íšê³¼ ì¶©ë¶„í•œ í…ŒìŠ¤íŠ¸ê°€ í•„ìˆ˜ì…ë‹ˆë‹¤. Flink 1.20ì€ **2ë…„ê°„ LTSë¡œ ì§€ì›**ë˜ë¯€ë¡œ, ê¸‰í•˜ì§€ ì•Šë‹¤ë©´ ì»¤ë„¥í„° ìƒíƒœê³„ê°€ ì•ˆì •í™”ë˜ëŠ” Flink 2.3 ì´í›„ ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ë¼ë©´ **Flink 2.x**ë¡œ ì‹œì‘í•˜ì—¬ ìµœì‹  APIì™€ ì•„í‚¤í…ì²˜ì˜ ì´ì ì„ í™œìš©í•˜ëŠ” ê²ƒì´ ê¶Œì¥ë©ë‹ˆë‹¤. íŠ¹íˆ Kubernetes í™˜ê²½ì—ì„œ ëŒ€ê·œëª¨ ìƒíƒœë¥¼ ë‹¤ë£¨ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì´ë¼ë©´ ForSt State Backendì˜ ì´ì ì´ í´ ê²ƒì…ë‹ˆë‹¤.