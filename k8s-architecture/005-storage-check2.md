좋습니다! CNPG(CloudNativePG)를 추가하겠습니다. 이 선택은 Block Storage 결정에도 영향을 주는 중요한 정보입니다.

-----

## 추가: CNPG (CloudNativePG)

### CNPG 개요

CloudNativePG는 Kubernetes 네이티브 PostgreSQL 오퍼레이터로, 다음 애플리케이션들의 메타데이터 DB로 사용될 수 있습니다:

|사용처     |용도                        |
|--------|--------------------------|
|Airflow |메타데이터, DAG 실행 이력, 연결 정보   |
|Grafana |대시보드, 사용자, 알림 설정 (HA 구성 시)|
|Superset|메타데이터 (향후 사용 시)           |
|MLflow  |실험 메타데이터 (향후 사용 시)        |
|기타 서비스  |애플리케이션별 PostgreSQL 필요 시   |

-----

### CNPG 스토리지 요구사항

|구성 요소                |스토리지 유형                                        |성능 요구    |용량           |비고                           |
|---------------------|-----------------------------------------------|---------|-------------|-----------------------------|
|PGDATA (데이터 디렉토리)    |**Local (High Performance)** 또는 **Block (RWO)**|**높음**   |수십 GB ~ 수백 GB|트랜잭션 처리, IOPS 중요             |
|WAL (Write-Ahead Log)|**Local (High Performance)**                   |**매우 높음**|수 GB ~ 수십 GB |순차 쓰기, 지연 시간 민감, 별도 볼륨 권장    |
|백업 (Base Backup)     |**Object (S3)**                                |낮음       |수십 GB ~ 수백 GB|Barman 통한 연속 백업              |
|WAL 아카이브             |**Object (S3)**                                |중간       |수십 GB        |PITR(Point-in-Time Recovery)용|

-----

### CNPG HA 아키텍처와 스토리지 관계

CNPG는 PostgreSQL 스트리밍 복제를 사용하여 HA를 구현합니다:

```
Primary (RW) ──복제──▶ Standby 1 (RO)
                   ──▶ Standby 2 (RO)
```

**핵심 포인트**: CNPG는 스토리지 레벨 복제가 아닌 **DB 레벨 복제**를 사용하므로, 분산 스토리지(Ceph 등) 없이 **LocalPV로도 HA 구성이 가능**합니다.

|장애 시나리오       |CNPG 동작                          |
|--------------|---------------------------------|
|Primary Pod 장애|Standby가 Primary로 자동 승격          |
|Primary 노드 장애 |Standby가 승격, 새 Standby는 다른 노드에 생성|
|Standby 장애    |새 Standby 자동 생성, Primary에서 데이터 복제|

-----

### Block Storage 결정에 대한 영향

CNPG 도입으로 인해 이전 “결정 1: Block Storage 제공 방식”을 재검토합니다:

|옵션                   |CNPG 관점에서의 평가                                      |
|---------------------|---------------------------------------------------|
|**A. LocalPV (NVMe)**|✅ **권장**. CNPG가 자체 복제로 HA 보장, NVMe 성능 활용 극대화, 구성 단순|
|**B. Rook-Ceph**     |가능하나 오버헤드. CNPG 복제 + Ceph 복제 = 이중 복제로 비효율          |
|**C. 외부 스토리지**       |가능하나 불필요. CNPG 복제만으로 충분                            |

**결론**: CNPG를 사용하면 **Block Storage로 LocalPV(NVMe)를 사용하는 것이 최적**입니다. 별도 분산 스토리지 구축이 불필요해집니다.

-----

## 수정된 매트릭스

|애플리케이션        |Local (NVMe)|Local (SAS)|Block (RWO)|File (RWX)|Object (S3)    |
|--------------|:----------:|:---------:|:---------:|:--------:|:-------------:|
|**Airflow**   |-           |✅ 임시       |-          |⚪ DAG     |✅ 로그           |
|**Spark**     |✅ Shuffle   |-          |-          |-         |✅ 데이터/CP       |
|**Trino**     |✅ Spill/캐시  |-          |-          |-         |✅ 데이터          |
|**Flink**     |✅ State     |-          |-          |-         |✅ CP/데이터       |
|**Kafka**     |✅ 로그        |-          |-          |-         |⚪ Tiered       |
|**StarRocks** |✅ BE 데이터    |✅ FE       |-          |-         |-              |
|**Prometheus**|✅ TSDB      |-          |-          |-         |⚪ 장기보관         |
|**Grafana**   |-           |✅ 기본       |-          |-         |-              |
|**OpenSearch**|✅ Hot       |✅ Warm     |-          |-         |✅ Cold/Snapshot|
|**로그 수집기**    |-           |✅ 버퍼       |-          |-         |-              |
|**CNPG**      |✅ PGDATA/WAL|-          |-          |-         |✅ 백업/아카이브      |

✅ 필수 / ⚪ 선택적

**변경 사항**:

- Airflow 메타DB가 Block(RWO)에서 제거됨 → CNPG가 LocalPV(NVMe)로 제공
- Grafana HA DB도 CNPG 사용 → Block(RWO) 불필요
- **Block (RWO) 컬럼이 사실상 불필요해짐**

-----

## 수정된 스토리지 유형별 사용처

|스토리지 유형                     |사용 애플리케이션/용도                                                                                                         |하드웨어 매핑           |
|----------------------------|---------------------------------------------------------------------------------------------------------------------|------------------|
|**Local (High Performance)**|Spark Shuffle, Flink State, Kafka 로그, Trino Spill, StarRocks BE, Prometheus TSDB, OpenSearch Hot, **CNPG PGDATA/WAL**|NVMe SSD 3.84T × 2|
|**Local (Standard)**        |OS, 컨테이너 런타임, Airflow Worker 임시, Grafana, OpenSearch Warm, 로그 수집기 버퍼, StarRocks FE                                   |SAS SSD 800G × 2  |
|**File (RWX)**              |Airflow DAG (Git-sync 대안)                                                                                            |Dell Isilon NAS   |
|**Object (S3)**             |데이터 I/O, Checkpoint, 로그, Prometheus 장기보관, OpenSearch Cold/Snapshot, **CNPG 백업/WAL 아카이브**                             |외부 MinIO          |

-----

## 수정된 결정 요청 사항

CNPG 추가로 인해 결정 사항이 단순화됩니다:

|#  |항목                   |옵션                                                 |비고                               |
|---|---------------------|---------------------------------------------------|---------------------------------|
|~1~|~Block Storage 제공 방식~|-                                                  |CNPG 사용으로 **결정 불필요** (LocalPV 사용)|
|2  |Airflow DAG 저장 방식    |A(Git-sync) / B(NFS) / C(Object Storage)           |                                 |
|3  |NVMe 파티셔닝 전략         |A(단일 용도) / B(용도별 분리) / C(노드 역할별)                   |                                 |
|4  |Prometheus 아키텍처      |A(Thanos) / B(Cortex) / C(Victoria Metrics) / D(샤딩)|                                 |
|5  |OpenSearch 노드 할당     |A(전용 노드) / B(혼합 배치)                                |                                 |

-----

## 최종 결정 요청

다음 4가지에 대해 결정해주시면 스토리지 아키텍처 상세 설계로 진행하겠습니다:

1. **Airflow DAG 저장 방식**: A(Git-sync) / B(NFS) / C(Object Storage)?
1. **NVMe 파티셔닝 전략**: A(단일 용도) / B(용도별 분리) / C(노드 역할별)?
1. **Prometheus 아키텍처**: A(Thanos) / B(Cortex) / C(Victoria Metrics) / D(샤딩)?
1. **OpenSearch 노드 할당**: A(전용 노드) / B(혼합 배치)?

추가 질문이나 고려사항 있으시면 말씀해주세요!​​​​​​​​​​​​​​​​

----------

좋은 질문들입니다! 하나씩 정리해드리겠습니다.

-----

## 1. Block Storage (Ceph) vs LocalPV 비교 분석

### 귀하의 사용 케이스 요약

|특성      |내용                                     |
|--------|---------------------------------------|
|주요 워크로드 |빅데이터 파이프라인 (Spark, Flink, Kafka, Trino)|
|데이터 특성  |대부분 임시 데이터 (Shuffle, State, Spill)     |
|영구 데이터  |MinIO(S3)에 저장, 메타DB는 CNPG로 해결          |
|DB HA 방식|CNPG 자체 복제 (스토리지 복제 불필요)               |

-----

### 비교 분석

|관점          |LocalPV (NVMe 직접 사용)       |Ceph (분산 스토리지)                     |
|------------|---------------------------|-----------------------------------|
|**성능**      |✅ NVMe 성능 100% 활용, 지연 시간 최소|❌ 네트워크 오버헤드, 3x 복제 쓰기 증폭           |
|**용량 효율**   |✅ 100% 사용 가능               |❌ 3x 복제 시 33%만 실사용 (7.68TB → 2.5TB)|
|**운영 복잡도**  |✅ 단순, Kubernetes 네이티브      |❌ Ceph 클러스터 별도 운영, 전문 지식 필요        |
|**장애 복구**   |Pod이 특정 노드에 바인딩됨           |✅ Pod이 다른 노드로 자유롭게 이동              |
|**동적 프로비저닝**|TopoLVM/OpenEBS로 가능        |✅ 네이티브 지원                          |
|**데이터 내구성** |노드 장애 시 로컬 데이터 손실          |✅ 3x 복제로 내구성 보장                    |

-----

### 귀하의 워크로드별 Ceph 필요성 분석

|워크로드                     |Ceph 필요?|이유                                |
|-------------------------|--------|----------------------------------|
|**Spark Shuffle**        |❌ 불필요   |임시 데이터, 작업 실패 시 재계산하면 됨           |
|**Flink State (RocksDB)**|❌ 불필요   |Checkpoint가 S3에 저장됨, State는 복구 가능 |
|**Kafka 로그**             |❌ 불필요   |Kafka 자체 복제 (replication factor=3)|
|**Trino Spill**          |❌ 불필요   |쿼리 실패 시 재실행하면 됨                   |
|**Prometheus TSDB**      |❌ 불필요   |Thanos가 S3에 장기 보관, 단기 데이터 손실 허용 가능|
|**OpenSearch Hot**       |⚪ 선택적   |복제본(replica) 설정으로 해결 가능           |
|**CNPG (PostgreSQL)**    |❌ 불필요   |스트리밍 복제 + S3 백업으로 HA/DR 해결        |
|**StarRocks BE**         |❌ 불필요   |자체 복제 메커니즘 있음                     |

**결론**: 귀하의 모든 워크로드가 **자체 복제 메커니즘** 또는 **외부 Object Storage 백업**을 통해 내구성을 확보합니다. Ceph의 스토리지 레벨 복제가 필요한 워크로드가 없습니다.

-----

### Ceph 도입이 의미 있는 경우 (참고)

|시나리오           |설명                          |
|---------------|----------------------------|
|전통적인 Stateful 앱|MySQL, MongoDB 등 자체 복제 없는 DB|
|RWX 블록 스토리지 필요 |여러 Pod이 동일 블록 볼륨 공유 (드문 케이스)|
|VM 워크로드        |KubeVirt 등으로 VM 운영 시        |
|레거시 마이그레이션     |기존 SAN/NAS 대체 필요 시          |

-----

### 최종 권장

|항목            |권장                                                     |
|--------------|-------------------------------------------------------|
|**Ceph 구축**   |❌ **불필요**                                              |
|**LocalPV 사용**|✅ **권장**                                               |
|**이유**        |모든 워크로드가 애플리케이션 레벨 복제 또는 S3 백업으로 내구성 확보, Ceph 오버헤드만 추가됨|

Ceph를 구축하면 운영 복잡도와 성능 손실만 발생하고, 실질적인 이점이 없습니다.

-----

## 2. NVMe 파티셔닝: 단일 용도 → 필요시 TopoLVM/OpenEBS

좋은 접근입니다! 이 전략에 대해 분석해드리겠습니다.

### 단계적 접근 전략

|단계                        |구성                                    |사용 시점                   |
|--------------------------|--------------------------------------|------------------------|
|**1단계: hostPath/emptyDir**|가장 단순, NVMe를 특정 경로에 마운트하고 직접 사용       |초기 구축, PoC              |
|**2단계: Static LocalPV**   |Kubernetes LocalPersistentVolume 수동 생성|노드-볼륨 바인딩 필요 시          |
|**3단계: TopoLVM/OpenEBS**  |동적 프로비저닝, 용량 관리, LVM 기반 파티셔닝          |여러 PVC가 하나의 NVMe 공유 필요 시|

-----

### TopoLVM vs OpenEBS LocalPV 비교

|관점          |TopoLVM                |OpenEBS LocalPV            |
|------------|-----------------------|---------------------------|
|**기반 기술**   |LVM                    |다양한 엔진 (LVM, ZFS, hostpath)|
|**동적 프로비저닝**|✅ LVM 기반               |✅ 지원                       |
|**용량 관리**   |✅ LVM thin provisioning|✅ 지원                       |
|**스냅샷**     |✅ LVM 스냅샷              |✅ 지원                       |
|**성숙도**     |CNCF Sandbox, 프로덕션 검증됨 |CNCF Sandbox, 널리 사용됨       |
|**복잡도**     |중간 (LVM 지식 필요)         |낮음 ~ 중간                    |
|**RHEL 호환성**|✅ LVM 네이티브             |✅ 호환                       |

-----

### 권장 전략

```
초기: hostPath 또는 Static LocalPV로 시작
  ↓
필요 시: TopoLVM 도입 (RHEL + LVM 친화적)
```

**TopoLVM 도입이 필요한 시점**:

- 하나의 NVMe를 여러 PVC로 나눠야 할 때
- 동적 프로비저닝이 운영 효율에 필수적일 때
- 용량 모니터링/관리 자동화가 필요할 때

**결론**: 단일 용도로 시작하고, 운영하면서 필요성이 생기면 TopoLVM 도입하는 전략에 **동의합니다**. ✅

-----

## 3. Thanos 추가된 매트릭스

### Thanos 스토리지 요구사항

|구성 요소           |스토리지 유형             |성능 요구|용량        |비고                                  |
|----------------|--------------------|-----|----------|------------------------------------|
|Sidecar         |-                   |-    |-         |Prometheus Pod에 사이드카로 배포, 별도 스토리지 없음|
|Store Gateway 캐시|**Local (Standard)**|중간   |수십 GB     |S3 데이터 로컬 캐싱                        |
|Compactor 임시 공간 |**Local (Standard)**|중간   |수십 ~ 수백 GB|다운샘플링/압축 작업용                        |
|Ruler (선택)      |**Local (Standard)**|낮음   |수 GB      |알림 규칙 평가 결과                         |
|장기 저장           |**Object (S3)**     |중간   |TB 단위     |핵심 저장소, 모든 메트릭 보관                   |

-----

### 수정된 전체 매트릭스

|애플리케이션        |Local (NVMe)|Local (SAS)          |File (RWX)|Object (S3)    |
|--------------|:----------:|:-------------------:|:--------:|:-------------:|
|**Airflow**   |-           |✅ 임시                 |-         |✅ 로그           |
|**Spark**     |✅ Shuffle   |-                    |-         |✅ 데이터/CP       |
|**Trino**     |✅ Spill/캐시  |-                    |-         |✅ 데이터          |
|**Flink**     |✅ State     |-                    |-         |✅ CP/데이터       |
|**Kafka**     |✅ 로그        |-                    |-         |⚪ Tiered       |
|**StarRocks** |✅ BE 데이터    |✅ FE                 |-         |-              |
|**Prometheus**|✅ TSDB/WAL  |-                    |-         |-              |
|**Thanos**    |-           |✅ Store 캐시, Compactor|-         |✅ 장기저장         |
|**Grafana**   |-           |✅ 기본                 |-         |-              |
|**OpenSearch**|✅ Hot       |✅ Warm               |-         |✅ Cold/Snapshot|
|**로그 수집기**    |-           |✅ 버퍼                 |-         |-              |
|**CNPG**      |✅ PGDATA/WAL|-                    |-         |✅ 백업/아카이브      |

✅ 필수 / ⚪ 선택적

-----

## 4. 관측성 스택 전용 노드 (5대) 분석

### 제안 구성

관측성 스택(Grafana, Thanos, Prometheus, OpenSearch)을 5대 전용 노드에 배치하는 방안입니다.

-----

### 리소스 요구사항 추정 (190대 모니터링 기준)

|구성 요소                     |CPU      |Memory   |디스크           |인스턴스 수            |
|--------------------------|---------|---------|--------------|------------------|
|**Prometheus**            |8~16 코어  |64~128 GB|NVMe 500GB~1TB|2대 (HA 또는 샤딩)     |
|**Thanos Sidecar**        |1~2 코어   |2~4 GB   |-             |Prometheus와 동일 Pod|
|**Thanos Query**          |4~8 코어   |8~16 GB  |-             |2대 (HA)           |
|**Thanos Store**          |4~8 코어   |16~32 GB |SAS 100GB (캐시)|2대 (HA)           |
|**Thanos Compactor**      |4~8 코어   |16~32 GB |SAS 200GB (임시)|1대                |
|**Grafana**               |2~4 코어   |4~8 GB   |SAS 10GB      |2대 (HA)           |
|**OpenSearch Master**     |4 코어     |16 GB    |SAS 50GB      |3대                |
|**OpenSearch Data (Hot)** |16~32 코어 |64~128 GB|NVMe 2~3TB    |3~5대              |
|**OpenSearch Data (Warm)**|8~16 코어  |32~64 GB |SAS 2~4TB     |2~3대 (선택)         |
|**Fluent Bit**            |0.5 코어/노드|256MB/노드 |-             |DaemonSet (전 노드)  |

-----

### 5대 노드로 충분한가?

|시나리오        |5대 가능 여부|설명                                                 |
|------------|--------|---------------------------------------------------|
|**최소 구성**   |⚠️ 빠듯함   |Prometheus 2 + Thanos 컴포넌트 + OpenSearch 3(마스터겸 데이터)|
|**권장 구성**   |❌ 부족    |OpenSearch 데이터 노드 분리 시 7~10대 필요                    |
|**여유 있는 구성**|❌ 부족    |향후 확장, Hot/Warm 분리 고려 시 10대 이상                     |

-----

### 노드 수별 구성 옵션

#### 옵션 A: 5대 (최소 구성)

|노드  |배치 구성 요소                                     |
|----|---------------------------------------------|
|노드 1|Prometheus + Thanos Sidecar + Thanos Query   |
|노드 2|Prometheus + Thanos Sidecar + Thanos Query   |
|노드 3|OpenSearch (Master + Data) + Grafana         |
|노드 4|OpenSearch (Master + Data) + Thanos Store    |
|노드 5|OpenSearch (Master + Data) + Thanos Compactor|

**장점**: 노드 수 최소화
**단점**: 리소스 경합, OpenSearch 성능 제한, 확장 여유 없음

-----

#### 옵션 B: 7대 (권장 구성)

|노드      |배치 구성 요소                          |
|--------|----------------------------------|
|노드 1-2  |Prometheus + Thanos Sidecar/Query |
|노드 3    |Thanos Store + Compactor + Grafana|
|노드 4-5-6|OpenSearch Master (전용, 경량)        |
|노드 7+   |OpenSearch Data (Hot)             |

잠깐, 이렇게 하면 7대 이상이 필요하네요. 다시 정리합니다.

-----

#### 옵션 B: 7대 (균형 구성)

|노드        |배치 구성 요소                                  |
|----------|------------------------------------------|
|노드 1      |Prometheus #1 + Thanos Sidecar            |
|노드 2      |Prometheus #2 + Thanos Sidecar            |
|노드 3      |Thanos Query + Store + Compactor + Grafana|
|노드 4-5-6-7|OpenSearch (Master 역할 3대 포함, Data 겸임)     |

**장점**: OpenSearch에 충분한 리소스, 역할 분리
**단점**: 2대 추가 필요

-----

#### 옵션 C: 10대 (여유 구성)

|노드      |배치 구성 요소                   |
|--------|---------------------------|
|노드 1-2  |Prometheus + Thanos Sidecar|
|노드 3-4  |Thanos Query + Store (HA)  |
|노드 5    |Thanos Compactor + Grafana |
|노드 6-7-8|OpenSearch Master (전용)     |
|노드 9-10 |OpenSearch Data Hot (확장 가능)|

**장점**: 완전한 역할 분리, 확장 용이, 장애 격리
**단점**: 노드 수 증가

-----

### 권장 사항

|항목    |권장                                                                           |
|------|-----------------------------------------------------------------------------|
|**최소**|7대                                                                           |
|**권장**|8~10대                                                                        |
|**이유**|OpenSearch가 190대에서 발생하는 로그를 처리하려면 충분한 리소스 필요. 5대는 초기에는 가능하나 금방 한계에 도달할 가능성 높음|

-----

### 로그 볼륨 추정 (참고)

|항목      |추정값                     |
|--------|------------------------|
|노드당 로그  |10~50 MB/분 (워크로드에 따라 다름)|
|190대 총량 |2~10 GB/분 = 3~15 TB/일   |
|7일 보관 시 |20~100 TB               |
|30일 보관 시|90~450 TB               |

이 볼륨을 처리하려면 OpenSearch Data 노드에 충분한 NVMe 용량과 CPU/Memory가 필요합니다.

-----

## 결정 요청

|#|항목                  |옵션                             |
|-|--------------------|-------------------------------|
|1|Block Storage (Ceph)|분석 결과 **불필요** - 동의하시나요?        |
|2|NVMe 전략             |단일 용도 → 필요시 TopoLVM ✅ 확정       |
|3|Thanos              |✅ 확정, 매트릭스 반영 완료               |
|4|관측성 노드 수            |A(5대-최소) / B(7대-권장) / C(10대-여유)|

4번에 대해 결정해주시면 스토리지 아키텍처 최종 정리로 넘어가겠습니다!​​​​​​​​​​​​​​​​

----------

## 스토리지 아키텍처 최종 정리 (확정)

-----

### 1. 스토리지 유형 정의 및 하드웨어 매핑

|스토리지 유형                     |하드웨어                      |용도                    |프로비저닝 방식              |
|----------------------------|--------------------------|----------------------|----------------------|
|**Local (High Performance)**|NVMe SSD 3.84TB × 2 (개별)  |고성능 I/O가 필요한 워크로드     |hostPath → 필요시 TopoLVM|
|**Local (Standard)**        |SAS SSD 800GB × 2 (RAID 1)|OS, 런타임, 임시 데이터, 캐시   |루트 파티션                |
|**File (RWX)**              |Dell Isilon NAS           |공유 파일시스템 (필요시)        |NFS CSI Driver        |
|**Object (S3)**             |외부 MinIO                  |데이터 레이크, 백업, 체크포인트, 로그|S3 API                |

-----

### 2. 디스크 구성

|디스크            |구성          |가용 용량  |마운트          |용도                             |
|---------------|------------|-------|-------------|-------------------------------|
|**SAS #1 + #2**|RAID 1 (미러링)|800 GB |`/` (루트)     |OS, 런타임, kubelet, 임시 데이터, 로그 버퍼|
|**NVMe #1**    |단일          |3.84 TB|`/data/nvme0`|워크로드별 주 데이터                    |
|**NVMe #2**    |단일          |3.84 TB|`/data/nvme1`|워크로드별 보조 데이터 또는 분리 용도          |

#### RAID 1 선택 근거

|항목             |내용                                            |
|---------------|----------------------------------------------|
|**목적**         |OS 디스크 장애 시에도 노드 운영 지속, 운영팀 대응 시간 확보          |
|**용량**         |800GB로 OS + 런타임 + 임시 데이터 충분 (예상 사용량 280~560GB)|
|**NVMe 미적용 사유**|모든 워크로드가 앱 레벨 복제 또는 S3 백업으로 내구성 확보            |

-----

### 3. 애플리케이션별 스토리지 요구사항

|애플리케이션        |Local (NVMe)    |Local (SAS RAID 1)   |Object (S3)            |비고                  |
|--------------|:--------------:|:-------------------:|:---------------------:|--------------------|
|**Airflow**   |-               |✅ Worker 임시          |✅ 태스크 로그               |DAG는 Git-sync       |
|**Spark**     |✅ Shuffle, Spill|-                    |✅ 데이터 I/O, Checkpoint  |Executor 로컬 디렉토리    |
|**Trino**     |✅ Spill, 캐시     |-                    |✅ Iceberg 데이터          |메모리 초과 시 디스크 사용     |
|**Flink**     |✅ RocksDB State |-                    |✅ Checkpoint, Savepoint|Stateful 처리 핵심      |
|**Kafka**     |✅ 로그 세그먼트       |-                    |⚪ Tiered Storage       |자체 복제로 내구성 확보       |
|**StarRocks** |✅ BE 데이터        |✅ FE 메타              |-                      |향후 도입               |
|**Prometheus**|✅ TSDB, WAL     |-                    |-                      |단기 보관 (2주 이내)       |
|**Thanos**    |-               |✅ Store 캐시, Compactor|✅ 장기 저장                |Prometheus 메트릭 장기 보관|
|**Grafana**   |-               |✅ 기본 저장소             |-                      |경량                  |
|**OpenSearch**|✅ Hot 데이터       |✅ Warm 데이터           |✅ Cold, Snapshot       |Hot-Warm-Cold 티어링   |
|**로그 수집기**    |-               |✅ 버퍼                 |-                      |Fluent Bit DaemonSet|
|**CNPG**      |✅ PGDATA, WAL   |-                    |✅ 백업, WAL 아카이브         |DB 레벨 복제로 HA        |

✅ 필수 / ⚪ 선택적

-----

### 4. 외부 스토리지 연동

|스토리지               |프로토콜  |용도                                                      |연동 방식               |
|-------------------|------|--------------------------------------------------------|--------------------|
|**MinIO**          |S3 API|데이터 레이크 (Iceberg), Checkpoint, 백업, 로그 아카이브, Thanos 장기 저장|각 앱에서 S3 endpoint 설정|
|**Dell Isilon NAS**|NFS   |RWX 볼륨 필요 시 (현재 사용 최소화)                                 |NFS CSI Driver (필요시)|

-----

### 5. 주요 결정 사항 요약

|항목                     |결정              |근거                        |
|-----------------------|----------------|--------------------------|
|**SAS SSD 구성**         |RAID 1 (미러링)    |OS 장애 대비, 무중단 운영, 800GB 충분|
|**NVMe SSD 구성**        |개별 디스크 (RAID 없음)|앱 레벨 복제/백업 존재, 성능 우선      |
|**분산 스토리지 (Ceph)**     |❌ 구축 안 함        |모든 워크로드가 앱 레벨 복제 또는 S3 백업 |
|**Block Storage (RWO)**|LocalPV 사용      |CNPG가 DB 복제 담당            |
|**Airflow DAG**        |Git-sync        |NFS 의존성 제거                |
|**NVMe 프로비저닝**         |단일 용도로 시작       |필요시 TopoLVM 도입            |
|**메트릭 장기 보관**          |Thanos + S3     |Prometheus 단기, Thanos 장기  |
|**관측성 노드**             |5대 전용           |노드 스펙(48C/768GB)으로 충분     |

-----

### 6. 인프라팀 확인 필요 사항

|항목          |질문                          |확인 이유                  |
|------------|----------------------------|-----------------------|
|RAID 컨트롤러   |서버에 하드웨어 RAID 컨트롤러가 있는가?    |없으면 mdadm 또는 LVM 미러링 고려|
|RAID 컨트롤러 모델|BBU(Battery Backup Unit) 유무?|정전 시 데이터 보호            |
|RAID 모니터링   |모니터링 도구 및 알림 설정 가능한가?       |장애 감지 필수               |

-----

### 7. 스토리지 아키텍처 다이어그램

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         Kubernetes Cluster (190대)                          │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌───────────────────────────────────────────────────────────────────────┐  │
│  │                         각 노드 디스크 구성                            │  │
│  │  ┌─────────────────────────────────────────────────────────────────┐  │  │
│  │  │  SAS #1 ─┬─ RAID 1 ─── /  (800GB)                               │  │  │
│  │  │  SAS #2 ─┘             └─ OS, Runtime, Temp, Log Buffer         │  │  │
│  │  ├─────────────────────────────────────────────────────────────────┤  │  │
│  │  │  NVMe #1 ───────────── /data/nvme0  (3.84TB)                    │  │  │
│  │  │                        └─ 주 데이터 (Shuffle, State, Log 등)     │  │  │
│  │  ├─────────────────────────────────────────────────────────────────┤  │  │
│  │  │  NVMe #2 ───────────── /data/nvme1  (3.84TB)                    │  │  │
│  │  │                        └─ 보조 데이터 또는 분리 용도              │  │  │
│  │  └─────────────────────────────────────────────────────────────────┘  │  │
│  └───────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐             │
│  │  Worker Nodes   │  │  Worker Nodes   │  │ Observability   │             │
│  │  (Spark/Flink)  │  │    (Kafka)      │  │    (5대)        │             │
│  ├─────────────────┤  ├─────────────────┤  ├─────────────────┤             │
│  │ NVMe: Shuffle,  │  │ NVMe: Log       │  │ NVMe: TSDB,     │             │
│  │       State     │  │       Segments  │  │       OpenSearch│             │
│  └─────────────────┘  └─────────────────┘  └─────────────────┘             │
│           │                   │                    │                        │
│           └───────────────────┴────────────────────┘                        │
│                               │                                             │
│                               ▼                                             │
│                      ┌─────────────────┐                                    │
│                      │   S3 API        │                                    │
│                      │  ─────────────  │                                    │
│                      │  • Data Lake    │                                    │
│                      │  • Checkpoints  │                                    │
│                      │  • Backups      │                                    │
│                      │  • Thanos       │                                    │
│                      │  • Log Archive  │                                    │
│                      └─────────────────┘                                    │
│                               │                                             │
└───────────────────────────────┼─────────────────────────────────────────────┘
                                │
                    ┌───────────┴───────────┐
                    │                       │
                    ▼                       ▼
          ┌─────────────────┐     ┌─────────────────┐
          │  외부 MinIO     │     │  Dell Isilon    │
          │  Cluster        │     │  NAS (예비)     │
          │  + Iceberg      │     │                 │
          └─────────────────┘     └─────────────────┘
```

-----

### 8. 다음 단계

스토리지 아키텍처가 확정되었습니다. 전체 로드맵 현황:

|단계|항목                |상태                         |
|--|------------------|---------------------------|
|1 |클러스터 아키텍처         |✅ 완료 (API LB만 네트워크 팀 답변 대기)|
|2 |네트워크 아키텍처         |⏸️ 네트워크 팀 답변 대기             |
|3 |스토리지 아키텍처         |✅ **확정**                   |
|4 |노드 역할 분리          |🔜 다음 진행 가능                 |
|5 |멀티테넌시 전략          |🔜                          |
|6 |워크로드별 고려사항        |🔜                          |
|7 |관측성(Observability)|🔜                          |

-----

**다음으로 “노드 역할 분리” 설계를 진행할까요?**

