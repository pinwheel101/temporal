좋습니다! 먼저 애플리케이션별 스토리지 요구사항을 분석한 후, 스토리지 아키텍처 설계로 넘어가겠습니다.

-----

## 애플리케이션별 스토리지 요구사항 분석

### 스토리지 유형 정의

|유형                          |설명               |접근 모드        |예시                      |
|----------------------------|-----------------|-------------|------------------------|
|**Local (High Performance)**|노드 로컬 NVMe, 최고 성능|단일 노드        |Shuffle, State, Kafka 로그|
|**Local (Standard)**        |노드 로컬 SAS SSD    |단일 노드        |OS, 컨테이너 이미지, 임시 데이터    |
|**Block (RWO)**             |네트워크 블록 스토리지     |ReadWriteOnce|데이터베이스, 단일 Pod 영구 데이터   |
|**File (RWX)**              |공유 파일시스템 (NFS)   |ReadWriteMany|여러 Pod에서 동시 접근 필요한 데이터  |
|**Object (S3)**             |Object Storage   |API 기반       |데이터 레이크, 체크포인트, 로그 아카이브 |

-----

### Airflow

|구성 요소                |스토리지 유형                      |성능 요구|용량    |비고                               |
|---------------------|-----------------------------|-----|------|---------------------------------|
|DAG 파일               |**File (RWX)** 또는 Git-sync   |낮음   |수 GB  |모든 Scheduler/Worker가 동일 DAG 접근 필요|
|태스크 로그               |**Object (S3)** 또는 File (RWX)|낮음   |수백 GB~|장기 보관, S3 권장                     |
|메타데이터 DB (PostgreSQL)|**Block (RWO)**              |중간   |수십 GB |트랜잭션 처리                          |
|Worker 임시 공간         |**Local (Standard)**         |중간   |가변    |XCom, 임시 파일                      |

-----

### Spark

|구성 요소         |스토리지 유형                     |성능 요구    |용량      |비고                    |
|--------------|----------------------------|---------|--------|----------------------|
|Shuffle 데이터   |**Local (High Performance)**|**매우 높음**|수백 GB/노드|작업 성능에 직접적 영향, NVMe 필수|
|Executor 임시 공간|**Local (High Performance)**|높음       |수백 GB/노드|Spill, 캐시             |
|Checkpoint    |**Object (S3)**             |중간       |가변      |Structured Streaming용 |
|Event 로그      |**Object (S3)**             |낮음       |수십 GB   |History Server용       |
|입출력 데이터       |**Object (S3)**             |높음       |TB 단위   |Iceberg/MinIO 연동      |

-----

### Trino

|구성 요소         |스토리지 유형                     |성능 요구    |용량      |비고                     |
|--------------|----------------------------|---------|--------|-----------------------|
|Spill 데이터     |**Local (High Performance)**|**매우 높음**|수백 GB/노드|메모리 초과 시 디스크로 spill    |
|쿼리 캐시         |**Local (High Performance)**|높음       |수백 GB/노드|선택적, 성능 향상용            |
|Catalog/Config|ConfigMap                   |-        |수 MB    |Kubernetes ConfigMap 사용|
|데이터 소스        |**Object (S3)**             |높음       |외부      |Iceberg 테이블 조회         |

-----

### Flink

|구성 요소        |스토리지 유형                     |성능 요구    |용량      |비고                      |
|-------------|----------------------------|---------|--------|------------------------|
|RocksDB State|**Local (High Performance)**|**매우 높음**|수백 GB/노드|Stateful 처리의 핵심, NVMe 필수|
|Checkpoint   |**Object (S3)**             |높음       |수백 GB~  |장애 복구용, 주기적 스냅샷         |
|Savepoint    |**Object (S3)**             |중간       |수백 GB~  |수동 스냅샷, 버전 업그레이드 시      |
|입출력 데이터      |**Object (S3)** / Kafka     |높음       |외부      |소스/싱크 연동                |

-----

### Kafka

|구성 요소         |스토리지 유형                     |성능 요구    |용량      |비고                         |
|--------------|----------------------------|---------|--------|---------------------------|
|로그 세그먼트       |**Local (High Performance)**|**매우 높음**|TB 단위/노드|핵심 데이터, NVMe 필수, 복제로 내구성 확보|
|KRaft 메타데이터   |**Local (High Performance)**|높음       |수 GB    |ZooKeeper 대체, 별도 디스크 권장    |
|Tiered Storage|**Object (S3)**             |중간       |TB 단위   |선택적, 콜드 데이터 오프로드           |

-----

### StarRocks (향후)

|구성 요소   |스토리지 유형                      |성능 요구    |용량      |비고       |
|--------|-----------------------------|---------|--------|---------|
|FE 메타데이터|**Local (Standard)** 또는 Block|중간       |수십 GB   |안정성 중요   |
|BE 데이터  |**Local (High Performance)** |**매우 높음**|TB 단위/노드|분석 성능의 핵심|
|캐시      |**Local (High Performance)** |높음       |수백 GB   |쿼리 가속    |

-----

## 요약: 스토리지 유형별 사용처

|스토리지 유형                     |사용 애플리케이션/용도                                                   |하드웨어 매핑           |
|----------------------------|---------------------------------------------------------------|------------------|
|**Local (High Performance)**|Spark Shuffle, Flink State, Kafka 로그, Trino Spill, StarRocks BE|NVMe SSD 3.84T × 2|
|**Local (Standard)**        |OS, 컨테이너 런타임, Airflow Worker 임시                                |SAS SSD 800G × 2  |
|**Block (RWO)**             |PostgreSQL (Airflow 메타DB), 기타 DB                               |결정 필요             |
|**File (RWX)**              |Airflow DAG (Git-sync 대안)                                      |Dell Isilon NAS   |
|**Object (S3)**             |모든 앱의 Checkpoint, 로그, 데이터 I/O                                  |외부 MinIO          |

-----

## 스토리지 요구사항 매트릭스

한눈에 보는 애플리케이션 × 스토리지 유형:

|애플리케이션       |Local (NVMe)|Local (SAS)|Block (RWO)|File (RWX)|Object (S3)|
|-------------|:----------:|:---------:|:---------:|:--------:|:---------:|
|**Airflow**  |-           |✅ 임시       |✅ 메타DB     |✅ DAG     |✅ 로그       |
|**Spark**    |✅ Shuffle   |-          |-          |-         |✅ 데이터/CP   |
|**Trino**    |✅ Spill/캐시  |-          |-          |-         |✅ 데이터      |
|**Flink**    |✅ State     |-          |-          |-         |✅ CP/데이터   |
|**Kafka**    |✅ 로그        |-          |-          |-         |⚪ Tiered   |
|**StarRocks**|✅ BE 데이터    |✅ FE       |-          |-         |-          |

✅ 필수 / ⚪ 선택적

-----

## 의사결정이 필요한 사항

분석 결과를 바탕으로 스토리지 아키텍처 설계 전 몇 가지 결정이 필요합니다:

### 결정 1: Block Storage (RWO) 제공 방식

Airflow 메타데이터 DB 등에 필요한 Block Storage를 어떻게 제공할지 결정이 필요합니다.

|옵션                                    |설명                     |장점                             |단점                                   |
|--------------------------------------|-----------------------|-------------------------------|-------------------------------------|
|**A. LocalPV (NVMe 일부 사용)**           |특정 노드의 NVMe를 DB 전용으로 사용|고성능, 추가 인프라 불필요                |Pod이 특정 노드에 고정됨, 노드 장애 시 수동 복구       |
|**B. 분산 스토리지 (Rook-Ceph)**            |NVMe 일부를 Ceph OSD로 구성  |동적 프로비저닝, 복제로 내구성 확보, Pod 이동 자유|구성 복잡, 오버헤드, 추가 관리 필요                |
|**C. 외부 스토리지 (Dell Isilon 또는 전용 SAN)**|외부 블록 스토리지 사용          |검증된 엔터프라이즈 솔루션                 |별도 인프라 필요, Isilon은 NFS 특화라 Block은 제한적|

**권장**: 메타DB 규모가 크지 않으므로 **옵션 A (LocalPV)**로 시작하고, HA가 필요하면 DB 레벨 복제(PostgreSQL Patroni 등)로 해결하는 것이 실용적입니다.

-----

### 결정 2: Airflow DAG 저장 방식

|옵션                         |설명                       |장점                             |단점                 |
|---------------------------|-------------------------|-------------------------------|-------------------|
|**A. Git-sync**            |각 Pod이 Git 저장소에서 DAG를 동기화|NFS 불필요, GitOps 패턴, 버전 관리 자연스러움|Git 서버 의존성, 동기화 지연 |
|**B. NFS (Dell Isilon)**   |공유 볼륨에 DAG 저장            |즉시 반영, 구성 단순                   |NFS 의존성, 장애 시 전체 영향|
|**C. Object Storage + 동기화**|S3에 DAG 저장, 주기적 동기화      |내구성 높음                         |구성 복잡, 지연 발생       |

**권장**: **옵션 A (Git-sync)**가 현대적인 베스트 프랙티스입니다. 이미 Git으로 DAG를 관리하신다면 자연스럽게 연동됩니다.

-----

### 결정 3: NVMe 파티셔닝 전략

노드당 NVMe 3.84TB × 2 = 7.68TB를 어떻게 나눌지 결정이 필요합니다.

|옵션                 |설명                                        |장점               |단점             |
|-------------------|------------------------------------------|-----------------|---------------|
|**A. 단일 용도**       |두 NVMe를 하나의 용도로 사용 (예: 전체를 Spark Shuffle용)|구성 단순, 최대 용량 활용  |유연성 낮음         |
|**B. 용도별 분리**      |NVMe 1: 애플리케이션 데이터, NVMe 2: 분산 스토리지 또는 예비 |용도별 격리, 장애 영향 최소화|용량 분산          |
|**C. 노드 역할별 다른 전략**|Kafka 노드는 전체 Kafka용, Spark 노드는 전체 Shuffle용|워크로드 최적화         |노드 역할 설계와 연계 필요|

**권장**: **옵션 C**가 가장 효율적입니다. 노드 역할 분리 설계와 함께 진행하면 좋습니다.

-----

## 결정 요청

다음 3가지에 대해 결정해주시면 스토리지 아키텍처 상세 설계로 넘어가겠습니다:

1. **Block Storage 제공 방식**: A(LocalPV) / B(Rook-Ceph) / C(외부 스토리지)?
1. **Airflow DAG 저장 방식**: A(Git-sync) / B(NFS) / C(Object Storage)?
1. **NVMe 파티셔닝 전략**: A(단일 용도) / B(용도별 분리) / C(노드 역할별)?

추가 질문이나 다른 고려사항이 있으시면 말씀해주세요!​​​​​​​​​​​​​​​​

-----

맞습니다! 죄송합니다. 관측성 스택의 스토리지 요구사항을 빠뜨렸네요. 추가해서 다시 정리해드리겠습니다.

-----

## 추가: 관측성 스택 스토리지 요구사항

### Prometheus

|구성 요소                |스토리지 유형                     |성능 요구    |용량        |비고                       |
|---------------------|----------------------------|---------|----------|-------------------------|
|TSDB (시계열 데이터)       |**Local (High Performance)**|**매우 높음**|수백 GB ~ TB|쓰기/읽기 모두 I/O 집약적, NVMe 권장|
|WAL (Write-Ahead Log)|**Local (High Performance)**|**매우 높음**|수십 GB     |TSDB와 동일 디스크 또는 분리 가능    |
|장기 보관 (선택)           |**Object (S3)**             |낮음       |TB 단위     |Thanos/Cortex 사용 시       |

**190대 규모 고려사항**

- 노드당 수백 개의 메트릭 × 190대 = 대량의 시계열 데이터
- 단일 Prometheus로는 한계 → Prometheus 샤딩 또는 Thanos/Victoria Metrics 고려 필요
- 보존 기간에 따라 용량 급증 (15일 보관 기준 수백 GB ~ 1TB 예상)

-----

### Grafana

|구성 요소                         |스토리지 유형                                |성능 요구|용량   |비고                      |
|------------------------------|---------------------------------------|-----|-----|------------------------|
|대시보드/설정 DB (SQLite/PostgreSQL)|**Block (RWO)** 또는 **Local (Standard)**|낮음   |수 GB |HA 구성 시 PostgreSQL 사용 권장|
|플러그인                          |**Local (Standard)**                   |낮음   |수백 MB|컨테이너 이미지에 포함 가능         |
|이미지 렌더링 캐시                    |**Local (Standard)**                   |낮음   |수 GB |알림 이미지 생성용              |

**참고**: Grafana 자체는 스토리지 부담이 적습니다. 데이터는 Prometheus/OpenSearch에서 조회합니다.

-----

### OpenSearch (로깅)

|구성 요소        |스토리지 유형                     |성능 요구    |용량      |비고                                 |
|-------------|----------------------------|---------|--------|-----------------------------------|
|데이터 노드 (Hot) |**Local (High Performance)**|**매우 높음**|TB 단위/노드|최근 로그, NVMe 필수                     |
|데이터 노드 (Warm)|**Local (Standard)**        |중간       |TB 단위/노드|오래된 로그, SAS SSD 가능                 |
|데이터 노드 (Cold)|**Object (S3)**             |낮음       |TB 단위   |아카이브용, Snapshot/Searchable Snapshot|
|마스터 노드       |**Local (Standard)**        |낮음       |수십 GB   |클러스터 메타데이터만                        |

**190대 규모 고려사항**

- 예상 로그량: 노드당 수십 MB/분 × 190대 = 수 GB/분
- 보존 기간과 인덱스 정책에 따라 수십 TB 필요 가능
- Hot-Warm-Cold 티어링 아키텍처 권장
- 전용 OpenSearch 노드 필요 (3~5대 이상)

-----

### 로그 수집기 (Fluent Bit / Fluentd / Vector)

|구성 요소|스토리지 유형             |성능 요구|용량     |비고                         |
|-----|--------------------|-----|-------|---------------------------|
|버퍼   |**Local (Standard)**|중간   |수 GB/노드|일시적 버퍼링, OpenSearch 장애 시 대비|

-----

## 수정된 요약: 스토리지 유형별 사용처

|스토리지 유형                     |사용 애플리케이션/용도                                                                                            |하드웨어 매핑           |
|----------------------------|--------------------------------------------------------------------------------------------------------|------------------|
|**Local (High Performance)**|Spark Shuffle, Flink State, Kafka 로그, Trino Spill, StarRocks BE, **Prometheus TSDB**, **OpenSearch Hot**|NVMe SSD 3.84T × 2|
|**Local (Standard)**        |OS, 컨테이너 런타임, Airflow Worker 임시, **Grafana**, **OpenSearch Warm**, **로그 수집기 버퍼**                        |SAS SSD 800G × 2  |
|**Block (RWO)**             |PostgreSQL (Airflow 메타DB), **Grafana DB (HA 시)**                                                        |결정 필요             |
|**File (RWX)**              |Airflow DAG (Git-sync 대안)                                                                               |Dell Isilon NAS   |
|**Object (S3)**             |모든 앱의 Checkpoint, 로그, 데이터 I/O, **Prometheus 장기보관**, **OpenSearch Cold/Snapshot**                        |외부 MinIO          |

-----

## 수정된 매트릭스

|애플리케이션        |Local (NVMe)|Local (SAS)|Block (RWO)|File (RWX)|Object (S3)    |
|--------------|:----------:|:---------:|:---------:|:--------:|:-------------:|
|**Airflow**   |-           |✅ 임시       |✅ 메타DB     |✅ DAG     |✅ 로그           |
|**Spark**     |✅ Shuffle   |-          |-          |-         |✅ 데이터/CP       |
|**Trino**     |✅ Spill/캐시  |-          |-          |-         |✅ 데이터          |
|**Flink**     |✅ State     |-          |-          |-         |✅ CP/데이터       |
|**Kafka**     |✅ 로그        |-          |-          |-         |⚪ Tiered       |
|**StarRocks** |✅ BE 데이터    |✅ FE       |-          |-         |-              |
|**Prometheus**|✅ TSDB      |-          |-          |-         |⚪ 장기보관         |
|**Grafana**   |-           |✅ 기본       |⚪ HA시 DB   |-         |-              |
|**OpenSearch**|✅ Hot       |✅ Warm     |-          |-         |✅ Cold/Snapshot|
|**로그 수집기**    |-           |✅ 버퍼       |-          |-         |-              |

✅ 필수 / ⚪ 선택적

-----

## 추가 의사결정 사항

### 결정 4: Prometheus 아키텍처

190대 규모에서 단일 Prometheus는 한계가 있습니다.

|옵션                        |설명                                   |장점                     |단점                     |
|--------------------------|-------------------------------------|-----------------------|-----------------------|
|**A. Prometheus + Thanos**|Prometheus 여러 대 + Thanos로 통합 쿼리/장기 저장|수평 확장, S3 장기 보관, 글로벌 뷰 |구성 복잡도 증가              |
|**B. Prometheus + Cortex**|멀티테넌시 지원 분산 Prometheus               |팀별 격리 용이, 수평 확장        |운영 복잡도 높음              |
|**C. Victoria Metrics**   |Prometheus 호환 고성능 TSDB               |단일 바이너리, 리소스 효율적, 운영 단순|Prometheus 생태계 일부 기능 제한|
|**D. 단일 Prometheus (샤딩)** |팀별 또는 역할별 Prometheus 분리              |구성 단순                  |글로벌 뷰 어려움, 관리 포인트 증가   |

**권장**: 190대 규모 + 4개 팀 멀티테넌시를 고려하면 **옵션 A (Thanos)** 또는 **옵션 C (Victoria Metrics)**가 적합합니다.

-----

### 결정 5: OpenSearch 노드 전용 할당

OpenSearch는 리소스를 많이 사용하므로 전용 노드 할당을 고려해야 합니다.

|옵션              |설명                      |장점             |단점                        |
|----------------|------------------------|---------------|--------------------------|
|**A. 전용 노드**    |OpenSearch 전용 노드 3~5대 할당|워크로드 격리, 안정적 성능|노드 수 감소 (185~187대 워커)     |
|**B. 워커 노드와 혼합**|다른 워크로드와 함께 배치          |리소스 활용도 높음     |리소스 경합, 로깅 장애 시 다른 워크로드 영향|

**권장**: **옵션 A (전용 노드)**를 권장합니다. 로깅은 모든 워크로드의 기반이므로 격리하는 것이 안전합니다.

-----

## 정리: 전체 결정 요청 사항

|#|항목                 |옵션                                                 |
|-|-------------------|---------------------------------------------------|
|1|Block Storage 제공 방식|A(LocalPV) / B(Rook-Ceph) / C(외부 스토리지)             |
|2|Airflow DAG 저장 방식  |A(Git-sync) / B(NFS) / C(Object Storage)           |
|3|NVMe 파티셔닝 전략       |A(단일 용도) / B(용도별 분리) / C(노드 역할별)                   |
|4|Prometheus 아키텍처    |A(Thanos) / B(Cortex) / C(Victoria Metrics) / D(샤딩)|
|5|OpenSearch 노드 할당   |A(전용 노드) / B(혼합 배치)                                |

결정해주시면 스토리지 아키텍처 상세 설계로 진행하겠습니다!​​​​​​​​​​​​​​​​